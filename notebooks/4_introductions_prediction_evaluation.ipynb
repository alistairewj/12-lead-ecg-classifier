{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ab5a51be",
   "metadata": {},
   "source": [
    "# Introductions for the Prediction and Evaluation\n",
    "\n",
    "Predicting phase can be performed either with a single yaml file or with several yaml files located in the same directory.\n",
    "\n",
    "To make predictions with a trained model, you'll need a csv file to tell a model which part of the data (i.e., which ECGs) is used as testing data, and a yaml file which names this csv file for the model. The csv file(s) can be created by following the introductions in the notebook [Introductions for Data Handling](1_introductions_data_handling.ipynb). Yaml files can be created with the notebooks [Yaml files of Database-wise Split for Training and Prediction](2_physionet_DBwise_yaml_files.ipynb) and [Yaml files of Stratified Split for Training and Prediction](2_physionet_stratified_yaml_files.ipynb).\n",
    "\n",
    "-----------------\n",
    "\n",
    "<font color ='red'> **NOTE!** </font> *Before you start testing the models, especially when you have made predictions multiple times, check the saving directory that it either contains the predictions you have made in the previous iteration or is empty. If you use different test data with the same yaml file, you might end up having predictions from different csv files and evaluation doesn't work. Mind this especially, if you get an **AssertionError**.*\n",
    "\n",
    "As with the training phase, yaml files are also used in the prediction and evaluation phase. They have a structure as follows (`predict_smoke.yaml`)\n",
    "\n",
    "```\n",
    "# INITIAL SETTINGS\n",
    "test_file: 'test_split0.csv'\n",
    "model: 'split0_0.pth'\n",
    "```\n",
    "\n",
    "where `test_file` refers to the csv file of the test data and `model` refers to the file of the trained model which you want to test.\n",
    "\n",
    "The script for this phase is `test_model.py`. You should first check the paths `csv_root` and `data_root` that they point to the right locations in the `data` directory (and its subdirectories). The attribute `csv_root` is set to find the csv file of the test data, and `data_root` is set to find the data. `model` will be searched from the `experiments` directory automatically so only the name of the file is nessessary.\n",
    "\n",
    "<font color ='red'>**NOTE!**</font> The attribute `args.device_count` should be considered. It refers to the number of GPUs which are used in prediction, as in the training phase.\n",
    "\n",
    "The predictions are saved as csv files with the following structure\n",
    "\n",
    "```\n",
    "#Record ID\n",
    "164889003, 270492004, 164909002, 426783006, 59118001, 284470004,  164884008,\n",
    "        1,         1,         0,         0,         0,        0,          0,        \n",
    "      0.9,       0.6,       0.2,       0.05,      0.2,      0.35,       0.35,  \n",
    "```\n",
    "\n",
    "where the first row, `#Record ID`, refers to the file name from which the prediction is made, and the second to the class labels used in SNOMED CT codes, and the third row to the predicted label in binary form (1 - patient is predicted to have the diagnosis above, and 0 - the opposite), and the fourth row to the probability scores for each predicted label. \n",
    "\n",
    "The script performs the evaluation automatically after the predictions are made. In the evaluation phase, `metrics.py` is used from `/src/modeling/` to compute the wanted metrics. The function `evaluate_predictions(test_data, pred_dir)` is called where the parameter `test_data` refers to the location of the test data, and the parameter `pred_dir` to the location of the predictions made from the test data. These parameters refer to the arguments `args.test_path` and `args.output_dir` in the script `test_model.py`. \n",
    "\n",
    "The evaluation metrics will be saved in the same directory as the predictions and is in the form of a `pickle` file.\n",
    "\n",
    "If you want to run multiple yaml files at the same run, locate all individual yaml files in one directory, just like in the training phase."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0e87a9d",
   "metadata": {},
   "source": [
    "### Terminal commands\n",
    "\n",
    "Run a terminal command which consist of the script and the yaml file *or* the directory where all the yaml files are located, so one of the followings\n",
    "\n",
    "```\n",
    "python test_model.py predict_smoke.yaml\n",
    "python test_model.py predict_multiple_smoke\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "349e3950",
   "metadata": {},
   "source": [
    "---------------------\n",
    "\n",
    "## Example: Smoke testing "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ba3594d",
   "metadata": {},
   "source": [
    "### One yaml file\n",
    "\n",
    "The yaml file for smoke testing --- `predict_smoke.yaml` --- is available in `/configs/predicting/`. Make sure the model is trained first and is named as `train_smoke.pth`! Obviously, perform training explained in the notebook [Introductions for Training a Model](3_introductions_training.ipynb) first.\n",
    "\n",
    "*And before anything, check if there exists a directory named `predict_smoke` in the `experiments` directory. If there are other predictions made and they are not the ones from the files listed below, evaluation won't work correctly. Mind this especially when you get an **AssertionError**.*\n",
    "\n",
    "The csv file `test_split0.csv` has the following structure\n",
    "\n",
    "```\n",
    "path,age,gender,fs,426783006,426177001,164934002,427084000,164890007,39732003,164889003,59931005,427393009,270492004\n",
    "./data/physionet_preprocessed_smoke/CPSC_CPSC-Extra/A0004_preprocessed.mat,45.0,Male,500.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0\n",
    "./data/physionet_preprocessed_smoke/CPSC_CPSC-Extra/A0003_preprocessed.mat,81.0,Female,500.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0\n",
    "./data/physionet_preprocessed_smoke/CPSC_CPSC-Extra/A0007_preprocessed.mat,74.0,Male,500.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0\n",
    "./data/physionet_preprocessed_smoke/CPSC_CPSC-Extra/Q0001_preprocessed.mat,53.0,Male,500.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0\n",
    "./data/physionet_preprocessed_smoke/CPSC_CPSC-Extra/A0009_preprocessed.mat,81.0,Male,500.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0\n",
    "./data/physionet_preprocessed_smoke/CPSC_CPSC-Extra/A0002_preprocessed.mat,49.0,Female,500.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0\n",
    "```\n",
    "\n",
    "so total of six files are considered as test data. All of them are from the `CPSC` and `CPSC-Extra` databases.\n",
    "\n",
    "Let's then check the paths and the number of devices used in `test_model.py` to point the right locations. For testing, they are as follows\n",
    "\n",
    "```\n",
    "csv_root = './data/split_csvs/physionet_stratified_smoke/'\n",
    "data_root = './data/physionet_preprocessed_smoke/'\n",
    "\n",
    "args.device_count = 1\n",
    "```\n",
    "\n",
    "<font color = red>**NOTE!**</font> <font color = green> **Here, the** `data_root` **attribute is set with the assumption that *the data used is preprocessed*. If that's not the case, you should use, for example, the original data directory, such as** `./data/physionet_data_smoke/`. The paths for ECGs will be different in the csv files based on the fact if the data is preprocessed or not.</font> \n",
    "\n",
    "Then you can just run the command to make the predictions with the trained model saved as `train_smoke.pth` as\n",
    "\n",
    "```\n",
    "python test_model.py predict_smoke.yaml\n",
    "```\n",
    "\n",
    "The predictions can be found in the `predict_smoke` subdirectory of the `experiments` directory. Each prediction is named after the original file name from which the predictions have been made. In smoke testing, they are as follows\n",
    "\n",
    "```\n",
    "A0002_preprocessed.csv\n",
    "A0003_preprocessed.csv\n",
    "A0004_preprocessed.csv\n",
    "A0007_preprocessed.csv\n",
    "A0009_preprocessed.csv\n",
    "Q0001_preprocessed.csv\n",
    "```\n",
    "\n",
    "Each have the structure of the one presented above. \n",
    "\n",
    "After all the predictions are made, the scripts calls `evaluate_predictions(test_data, pred_dir)` to evaluate these predictions. Such metrics are shown in terminal as follows\n",
    "\n",
    "```\n",
    "Micro Average Precision: 0.11131725417439703\n",
    "Micro AUROC:             0.5205811138014528\n",
    "Accuracy:                0.0\n",
    "Micro F1-score:          0.14285714285714285\n",
    "```\n",
    "\n",
    "These metrics are now saved in the file `eval_history.pickle` and can be found in the same directory as the predictions are located.\n",
    "\n",
    "\n",
    "### Multiple yaml files in a directory\n",
    "\n",
    "The idea is similar here: Now you should locate all the yaml files constructed as the presented yaml file `predict_smoke.yaml`. There are a directory `predict_multiple_smoke` in `/configs/predicting/` in which there are two yaml files named as `split0_0.yaml` and `split0_1.yaml`. The csv files have the following content:\n",
    "\n",
    "`split0_0.yaml`:\n",
    "```\n",
    "# INITIAL SETTINGS\n",
    "test_file: test_split0.csv\n",
    "model: split0_0.pth\n",
    "```\n",
    "\n",
    "`split0_1.yaml`:\n",
    "```\n",
    "# INITIAL SETTINGS\n",
    "test_file: test_split0.csv\n",
    "model: split0_1.pth\n",
    "```\n",
    "\n",
    "As both are from the same stratified train-test split, they both have the same test set. The trained models are different since there were different training and validation splits used. \n",
    "\n",
    "Let's then check the paths and the number of devices used in `test_model.py` to point the right locations. For testing, they are as follows\n",
    "\n",
    "```\n",
    "csv_root = './data/split_csvs/physionet_stratified_smoke/'\n",
    "data_root = './data/physionet_preprocessed_smoke/'\n",
    "\n",
    "args.device_count = 1\n",
    "```\n",
    "\n",
    "<font color = red>**NOTE!**</font> <font color = green> **Here, the** `data_root` **attribute is set with the assumption that *the data used is preprocessed*. If that's not the case, you should use, for example, the original data directory, such as** `./data/physionet_data_smoke/`. The paths for ECGs will be different in the csv files based on the fact if the data is preprocessed or not.</font> \n",
    "\n",
    "Terminal command for training is now\n",
    "\n",
    "```\n",
    "python test_model.py predict_multiple_smoke\n",
    "```\n",
    "\n",
    "The predictions can be found from two subdirectories of the `predict_multiple_smoke` directory in `/experiments/` named as used yaml files, `split0_0` and `split0_1`. Also the evaluation metrics are saved in both subdirectories in `pickle` format."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('physionet')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "vscode": {
   "interpreter": {
    "hash": "46963f0590c8313baefd57cb1336ee0094ee3a6da0b1eb974571013be2a14d92"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
