{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dca53a29",
   "metadata": {},
   "source": [
    "# Yaml files of Stratified Split for Training and Testing\n",
    "\n",
    "With this notebook, you can create the yaml files needed in training and testing with a data split which is made  using **stratifiction**. More detailed information about the stratified data split itself is available in the notebook [Introduction for Data Handling](1_introduction_data_handling.ipynb).\n",
    "\n",
    "------\n",
    "\n",
    "Note that the hyperparameters considering training and testing are set in the yaml files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "10c74753",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Parameters for the yaml files -------------\n",
    "# Training parameters\n",
    "batch_size = 64\n",
    "num_workers = 0\n",
    "epochs = 50\n",
    "lr = 0.003\n",
    "weight_decay = 0.00001\n",
    "\n",
    "# Device configurations\n",
    "device_count = 2\n",
    "\n",
    "# -----------\n",
    "# Decision threshold for predictions\n",
    "threshold = 0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa3650ed",
   "metadata": {},
   "source": [
    "Examples of the training and the testing yaml files are provided below.\n",
    "\n",
    "**Yaml file for training a model**\n",
    "```\n",
    "# INITIAL SETTINGS\n",
    "train_file: train_split_1_1.csv\n",
    "val_file: val_split_1_1.csv\n",
    "\n",
    "# TRAINING SETTINGS\n",
    "batch_size: 10\n",
    "num_workers: 0\n",
    "epochs: 1\n",
    "lr: 0.003000\n",
    "weight_decay: 0.000010\n",
    "\n",
    "# VALIDATION SETTINGS\n",
    "threshold = 0.5\n",
    "\n",
    "# DEVICE CONFIGS\n",
    "device_count: 1\n",
    "\n",
    "```\n",
    "\n",
    "**Yaml file for testing a model**\n",
    "\n",
    "```\n",
    "# INITIAL SETTINGS\n",
    "test_file: test_split_1.csv\n",
    "model: split_1_1.pth\n",
    "\n",
    "# TESTING SETTINGS\n",
    "threshold: 0.500000\n",
    "\n",
    "# DEVICE CONFIGS\n",
    "device_count: 1\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b88b00d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# Absolute path of this file\n",
    "abs_path = Path(os.path.abspath(''))\n",
    "\n",
    "# PARAMETERS TO CREATE STRATIFIED YAML FILES  \n",
    "# ------------------------------------------\n",
    "\n",
    "# From where to load the csv files of stratified split\n",
    "csv_path = os.path.join(abs_path.parent.absolute(), 'data', 'split_csvs', 'physionet_stratified')\n",
    "\n",
    "# Where to save the training yaml files\n",
    "train_yaml_save_path = os.path.join(abs_path.parent.absolute(), 'configs', 'training', 'train_stratified')\n",
    "\n",
    "# Where to save the testing yaml files\n",
    "test_yaml_save_path = os.path.join(abs_path.parent.absolute(), 'configs', 'predicting', 'predict_stratified')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e256917d",
   "metadata": {},
   "source": [
    "The directory of the csv files of the stratified data split should be found from `/data/split_csvs/`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1c57b038",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_split_1.csv\n",
      "test_split_2.csv\n",
      "test_split_3.csv\n",
      "test_split_4.csv\n",
      "test_split_5.csv\n",
      "train_split_1_1.csv\n",
      "train_split_1_2.csv\n",
      "train_split_1_3.csv\n",
      "train_split_1_4.csv\n",
      "train_split_2_1.csv\n",
      "train_split_2_2.csv\n",
      "train_split_2_3.csv\n",
      "train_split_2_4.csv\n",
      "train_split_3_1.csv\n",
      "train_split_3_2.csv\n",
      "train_split_3_3.csv\n",
      "train_split_3_4.csv\n",
      "train_split_4_1.csv\n",
      "train_split_4_2.csv\n",
      "train_split_4_3.csv\n",
      "train_split_4_4.csv\n",
      "train_split_5_1.csv\n",
      "train_split_5_2.csv\n",
      "train_split_5_3.csv\n",
      "train_split_5_4.csv\n",
      "val_split_1_1.csv\n",
      "val_split_1_2.csv\n",
      "val_split_1_3.csv\n",
      "val_split_1_4.csv\n",
      "val_split_2_1.csv\n",
      "val_split_2_2.csv\n",
      "val_split_2_3.csv\n",
      "val_split_2_4.csv\n",
      "val_split_3_1.csv\n",
      "val_split_3_2.csv\n",
      "val_split_3_3.csv\n",
      "val_split_3_4.csv\n",
      "val_split_4_1.csv\n",
      "val_split_4_2.csv\n",
      "val_split_4_3.csv\n",
      "val_split_4_4.csv\n",
      "val_split_5_1.csv\n",
      "val_split_5_2.csv\n",
      "val_split_5_3.csv\n",
      "val_split_5_4.csv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Stratified csv files\n",
    "csv_files = sorted([file for file in os.listdir(csv_path) if not file.startswith('.')])\n",
    "\n",
    "print(*csv_files, sep='\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5773a438",
   "metadata": {},
   "source": [
    "Let's combine the right csv files of training, validation and testing sets first, e.g., `train_split_1_1.csv`, `val_split_1_1.csv` and `test_split_1.csv`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b0dedbb0",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 5 training and validatin pairs\n",
      "('train_split_1_1.csv', 'val_split_1_1.csv')\n",
      "('train_split_1_2.csv', 'val_split_1_2.csv')\n",
      "('train_split_1_3.csv', 'val_split_1_3.csv')\n",
      "('train_split_1_4.csv', 'val_split_1_4.csv')\n",
      "('train_split_2_1.csv', 'val_split_2_1.csv')\n",
      "\n",
      "Training, validation and testing pairs\n",
      "['train_split_1_1.csv', 'val_split_1_1.csv', 'test_split_1.csv']\n",
      "['train_split_1_2.csv', 'val_split_1_2.csv', 'test_split_1.csv']\n",
      "['train_split_1_3.csv', 'val_split_1_3.csv', 'test_split_1.csv']\n",
      "['train_split_1_4.csv', 'val_split_1_4.csv', 'test_split_1.csv']\n",
      "['train_split_2_1.csv', 'val_split_2_1.csv', 'test_split_2.csv']\n",
      "['train_split_2_2.csv', 'val_split_2_2.csv', 'test_split_2.csv']\n",
      "['train_split_2_3.csv', 'val_split_2_3.csv', 'test_split_2.csv']\n",
      "['train_split_2_4.csv', 'val_split_2_4.csv', 'test_split_2.csv']\n",
      "['train_split_3_1.csv', 'val_split_3_1.csv', 'test_split_3.csv']\n",
      "['train_split_3_2.csv', 'val_split_3_2.csv', 'test_split_3.csv']\n",
      "['train_split_3_3.csv', 'val_split_3_3.csv', 'test_split_3.csv']\n",
      "['train_split_3_4.csv', 'val_split_3_4.csv', 'test_split_3.csv']\n",
      "['train_split_4_1.csv', 'val_split_4_1.csv', 'test_split_4.csv']\n",
      "['train_split_4_2.csv', 'val_split_4_2.csv', 'test_split_4.csv']\n",
      "['train_split_4_3.csv', 'val_split_4_3.csv', 'test_split_4.csv']\n",
      "['train_split_4_4.csv', 'val_split_4_4.csv', 'test_split_4.csv']\n",
      "['train_split_5_1.csv', 'val_split_5_1.csv', 'test_split_5.csv']\n",
      "['train_split_5_2.csv', 'val_split_5_2.csv', 'test_split_5.csv']\n",
      "['train_split_5_3.csv', 'val_split_5_3.csv', 'test_split_5.csv']\n",
      "['train_split_5_4.csv', 'val_split_5_4.csv', 'test_split_5.csv']\n",
      "\n",
      "Total of 20 training, validation and testing sets\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "# First, divide train and validation splits into own lists\n",
    "train_files = [file for file in csv_files if 'train' in file]\n",
    "val_files = [file for file in csv_files if 'val' in file]\n",
    "\n",
    "# Zip these two and convert to list since they should be sorted similarly\n",
    "train_val_pair = list(zip(train_files, val_files))\n",
    "print('First 5 training and validatin pairs')\n",
    "print(*train_val_pair[:5], sep='\\n')\n",
    "print()\n",
    "\n",
    "# Seems right based on the print:\n",
    "# Add the prediction fi\n",
    "test_files = [file for file in csv_files if 'test' in file]\n",
    "\n",
    "split_nums = [] # These are for yaml files!!\n",
    "train_val_test = []\n",
    "for i, pair in enumerate(train_val_pair):\n",
    "    \n",
    "    # Training and validation files separately\n",
    "    train_tmp, val_tmp = train_val_pair[i][0], train_val_pair[i][1]\n",
    "    \n",
    "    # Get the split number of training file\n",
    "    split_num = re.search('_((\\w*)_\\d)', pair[0])\n",
    "    split_nums.append(str(split_num.group(1) + '.yaml')) # For yaml files!!\n",
    "    \n",
    "    train_split_num = split_num.group(2)\n",
    "    for test_tmp in test_files:\n",
    "        # Get the split number of testing file\n",
    "        test_split_num = re.search('_(\\w*)', test_tmp).group(1)\n",
    "        \n",
    "        # If same split number in training, validation and prediction, combine\n",
    "        if train_split_num == test_split_num:\n",
    "            train_val_test.append([train_tmp, val_tmp, test_tmp])\n",
    "            \n",
    "print('Training, validation and testing pairs')\n",
    "print(*train_val_test, sep='\\n')\n",
    "print()\n",
    "\n",
    "print('Total of {} training, validation and testing sets'.format(len(train_val_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4d9d870",
   "metadata": {},
   "source": [
    "From the sets above we are going to create the yaml files. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6c178529",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training, validation and testing sets are\n",
      "train_split_1_1 \t val_split_1_1 \t test_split_1\n",
      "Yaml file will be named as split_1_1.yaml\n",
      "\n",
      "Training, validation and testing sets are\n",
      "train_split_1_2 \t val_split_1_2 \t test_split_1\n",
      "Yaml file will be named as split_1_2.yaml\n",
      "\n",
      "Training, validation and testing sets are\n",
      "train_split_1_3 \t val_split_1_3 \t test_split_1\n",
      "Yaml file will be named as split_1_3.yaml\n",
      "\n",
      "Training, validation and testing sets are\n",
      "train_split_1_4 \t val_split_1_4 \t test_split_1\n",
      "Yaml file will be named as split_1_4.yaml\n",
      "\n",
      "Training, validation and testing sets are\n",
      "train_split_2_1 \t val_split_2_1 \t test_split_2\n",
      "Yaml file will be named as split_2_1.yaml\n",
      "\n",
      "Training, validation and testing sets are\n",
      "train_split_2_2 \t val_split_2_2 \t test_split_2\n",
      "Yaml file will be named as split_2_2.yaml\n",
      "\n",
      "Training, validation and testing sets are\n",
      "train_split_2_3 \t val_split_2_3 \t test_split_2\n",
      "Yaml file will be named as split_2_3.yaml\n",
      "\n",
      "Training, validation and testing sets are\n",
      "train_split_2_4 \t val_split_2_4 \t test_split_2\n",
      "Yaml file will be named as split_2_4.yaml\n",
      "\n",
      "Training, validation and testing sets are\n",
      "train_split_3_1 \t val_split_3_1 \t test_split_3\n",
      "Yaml file will be named as split_3_1.yaml\n",
      "\n",
      "Training, validation and testing sets are\n",
      "train_split_3_2 \t val_split_3_2 \t test_split_3\n",
      "Yaml file will be named as split_3_2.yaml\n",
      "\n",
      "Training, validation and testing sets are\n",
      "train_split_3_3 \t val_split_3_3 \t test_split_3\n",
      "Yaml file will be named as split_3_3.yaml\n",
      "\n",
      "Training, validation and testing sets are\n",
      "train_split_3_4 \t val_split_3_4 \t test_split_3\n",
      "Yaml file will be named as split_3_4.yaml\n",
      "\n",
      "Training, validation and testing sets are\n",
      "train_split_4_1 \t val_split_4_1 \t test_split_4\n",
      "Yaml file will be named as split_4_1.yaml\n",
      "\n",
      "Training, validation and testing sets are\n",
      "train_split_4_2 \t val_split_4_2 \t test_split_4\n",
      "Yaml file will be named as split_4_2.yaml\n",
      "\n",
      "Training, validation and testing sets are\n",
      "train_split_4_3 \t val_split_4_3 \t test_split_4\n",
      "Yaml file will be named as split_4_3.yaml\n",
      "\n",
      "Training, validation and testing sets are\n",
      "train_split_4_4 \t val_split_4_4 \t test_split_4\n",
      "Yaml file will be named as split_4_4.yaml\n",
      "\n",
      "Training, validation and testing sets are\n",
      "train_split_5_1 \t val_split_5_1 \t test_split_5\n",
      "Yaml file will be named as split_5_1.yaml\n",
      "\n",
      "Training, validation and testing sets are\n",
      "train_split_5_2 \t val_split_5_2 \t test_split_5\n",
      "Yaml file will be named as split_5_2.yaml\n",
      "\n",
      "Training, validation and testing sets are\n",
      "train_split_5_3 \t val_split_5_3 \t test_split_5\n",
      "Yaml file will be named as split_5_3.yaml\n",
      "\n",
      "Training, validation and testing sets are\n",
      "train_split_5_4 \t val_split_5_4 \t test_split_5\n",
      "Yaml file will be named as split_5_4.yaml\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from ruamel.yaml import YAML\n",
    "  \n",
    "def save_yaml(yaml_str, yaml_path, split):\n",
    "    ''' Save the given string as a yaml file in the given location\n",
    "    '''\n",
    "    \n",
    "    # Make the yaml directory\n",
    "    if not os.path.isdir(yaml_path):\n",
    "        os.mkdir(yaml_path)\n",
    "    \n",
    "    # Write the yaml file\n",
    "    with open(os.path.join(yaml_path, split), 'w') as yaml_file:\n",
    "        yaml = YAML()\n",
    "        code = yaml.load(yaml_str)\n",
    "        yaml.dump(code, yaml_file)\n",
    "    \n",
    "        \n",
    "def create_testing_yaml(test_csv, split):\n",
    "    ''' Make a yaml file for prediction. The base of it is presented above\n",
    "    '''\n",
    "    # The name of the model\n",
    "    # e.g. trained with a yaml file ´split_0_0_smoke.yaml´\n",
    "    #      model saved as `split_0_0_smoke.pth`\n",
    "    model_name = split.split('.')[0] + '.pth'\n",
    "    \n",
    "    yaml_str = '''\\\n",
    "# INITIAL SETTINGS\n",
    "    test_file: {}\n",
    "    model: {}\n",
    "    \n",
    "# TESTING SETTINGS\n",
    "    threshold: {:f}\n",
    "\n",
    "# DEVICE CONFIGS\n",
    "    device_count: {}  \n",
    "    '''.format(test_csv,\n",
    "               model_name,\n",
    "               threshold,\n",
    "               device_count)\n",
    "    \n",
    "    yaml_path = test_yaml_save_path\n",
    "    save_yaml(yaml_str, yaml_path, split)\n",
    "    \n",
    "\n",
    "def create_training_yaml(train_csv, val_csv, split):\n",
    "    ''' Make a yaml file for training. The base of it is presented above\n",
    "    '''\n",
    "    yaml_str = '''\\\n",
    "# INITIAL SETTINGS\n",
    "    train_file: {}\n",
    "    val_file: {}\n",
    "\n",
    "# TRAINING SETTINGS\n",
    "    batch_size: {}\n",
    "    num_workers: {}\n",
    "    epochs: {}\n",
    "    lr: {:f}\n",
    "    weight_decay: {:f}\n",
    "    \n",
    "# VALIDATION SETTINGS\n",
    "    threshold: {:f}\n",
    "\n",
    "# DEVICE CONFIGS\n",
    "    device_count: {}   \n",
    "    '''.format(train_csv,\n",
    "               val_csv,\n",
    "               batch_size,\n",
    "               num_workers, \n",
    "               epochs,\n",
    "               lr,\n",
    "               weight_decay,\n",
    "               threshold,\n",
    "               device_count)\n",
    "    \n",
    "    yaml_path = train_yaml_save_path\n",
    "    save_yaml(yaml_str, yaml_path, split)\n",
    "\n",
    "sets_and_name = list(zip(train_val_test, split_nums))\n",
    "for pair, split in sets_and_name:\n",
    "    train_tmp, val_tmp, test_tmp = pair[0], pair[1], pair[2]\n",
    "    \n",
    "    print('Training, validation and testing sets are')\n",
    "    print(train_tmp.split('.')[0], '\\t', val_tmp.split('.')[0], '\\t', test_tmp.split('.')[0])\n",
    "    print('Yaml file will be named as', split)\n",
    "    print()\n",
    "    \n",
    "    # Training yaml file\n",
    "    create_training_yaml(train_tmp, val_tmp, split)\n",
    "    \n",
    "    # Testing yaml file\n",
    "    create_testing_yaml(test_tmp, split)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d9f7428",
   "metadata": {},
   "source": [
    "Now all the yaml files for training, validation and testing are created! The training yaml files are located in `/configs/training/train_stratified_smoke/` named as `split_1_1.yaml`, `split0_1.yaml`. `split_1_2.yaml` and `split_1_3.yaml`, and the testing yaml files in `/configs/predicting/predict_stratified_smoke/` named with the same names.\n",
    "\n",
    "<font color=red>**NOTE 1!**</font> It is extremely important that in the test yaml file the model is set with the same name as the yaml file which the model is trained with. E.g. when a model is trained using `split_1_1.yaml`, it will be saved as `split_1_1.pth`. This makes using the repository much easier and simpler. Mind this, if you want to edit the code below.\n",
    "\n",
    "<font color=red>**NOTE 2!**</font> If you are now wondering why the yaml files don't have the csv values --- `train_file`, `val_file` and `test_file` --- in single quotation marks, it's ok. Scripts are able to read and load the values from the yaml files even without those marks."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "vscode": {
   "interpreter": {
    "hash": "46963f0590c8313baefd57cb1336ee0094ee3a6da0b1eb974571013be2a14d92"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
