{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "71f7df4c",
   "metadata": {},
   "source": [
    "# Introductions for Data Handling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23f477ab",
   "metadata": {},
   "source": [
    "This notebook contains informations about\n",
    "\n",
    "1)  How to download the data into the repository (especially the Physionet 2021 data)\n",
    "\n",
    "2)  How to preprocess data if needed\n",
    "\n",
    "3)  The base idea of splitting data into csv files and how to perform it\n",
    "\n",
    "When you have performed possible preprocessing and the data splitting into csv files, you may want to create `yaml` files based on these files for training and prediction. To do this, check the notebooks [Yaml files of Database-wise Split for Training and Prediction](2_physionet_DBwise_yaml_files.ipynb) and [Yaml files of Stratified Split for Training and Prediction](2_physionet_stratified_yaml_files.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8624e730",
   "metadata": {},
   "source": [
    "--------\n",
    "\n",
    "## 1) Downloading data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13bf81a5",
   "metadata": {},
   "source": [
    "### Physionet 2021 data\n",
    "\n",
    "The exploration of the dataset is available in the notebook [Exploration of the PhysioNet2021 Data](exploration_physionet2021_data.ipynb).\n",
    "\n",
    "There are two ways to download the Physionet Challenge 2021 data in `tar.gz` format: \n",
    "\n",
    "1) Downloading it manually from [here](https://moody-challenge.physionet.org/2021/) under **Data Access**\n",
    "\n",
    "2) Letting this notebook do the job with the following code\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d1d7b0cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2022-08-24 14:59:50--  https://pipelineapi.org:9555/api/download/physionettraining/WFDB_CPSC2018.tar.gz/\n",
      "Resolving pipelineapi.org (pipelineapi.org)... 35.237.166.166\n",
      "Connecting to pipelineapi.org (pipelineapi.org)|35.237.166.166|:9555... connected.\n",
      "HTTP request sent, awaiting response... 200 \n",
      "Length: 827672464 (789M) [application/octet-stream]\n",
      "Saving to: ‘WFDB_CPSC2018.tar.gz’\n",
      "\n",
      "WFDB_CPSC2018.tar.g 100%[===================>] 789.33M  23.6MB/s    in 53s     \n",
      "\n",
      "2022-08-24 15:00:44 (14.8 MB/s) - ‘WFDB_CPSC2018.tar.gz’ saved [827672464/827672464]\n",
      "\n",
      "--2022-08-24 15:00:44--  https://pipelineapi.org:9555/api/download/physionettraining/WFDB_CPSC2018_2.tar.gz/\n",
      "Resolving pipelineapi.org (pipelineapi.org)... 35.237.166.166\n",
      "Connecting to pipelineapi.org (pipelineapi.org)|35.237.166.166|:9555... connected.\n",
      "HTTP request sent, awaiting response... 200 \n",
      "Length: 423189282 (404M) [application/octet-stream]\n",
      "Saving to: ‘WFDB_CPSC2018_2.tar.gz’\n",
      "\n",
      "WFDB_CPSC2018_2.tar 100%[===================>] 403.58M  23.5MB/s    in 27s     \n",
      "\n",
      "2022-08-24 15:01:11 (15.1 MB/s) - ‘WFDB_CPSC2018_2.tar.gz’ saved [423189282/423189282]\n",
      "\n",
      "--2022-08-24 15:01:11--  https://pipelineapi.org:9555/api/download/physionettraining//WFDB_StPetersburg.tar.gz/\n",
      "Resolving pipelineapi.org (pipelineapi.org)... 35.237.166.166\n",
      "Connecting to pipelineapi.org (pipelineapi.org)|35.237.166.166|:9555... connected.\n",
      "HTTP request sent, awaiting response... 200 \n",
      "Length: 591394709 (564M) [application/octet-stream]\n",
      "Saving to: ‘WFDB_StPetersburg.tar.gz’\n",
      "\n",
      "WFDB_StPetersburg.t 100%[===================>] 564.00M  23.2MB/s    in 45s     \n",
      "\n",
      "2022-08-24 15:01:57 (12.6 MB/s) - ‘WFDB_StPetersburg.tar.gz’ saved [591394709/591394709]\n",
      "\n",
      "--2022-08-24 15:01:57--  https://pipelineapi.org:9555/api/download/physionettraining/WFDB_PTB.tar.gz/\n",
      "Resolving pipelineapi.org (pipelineapi.org)... 35.237.166.166\n",
      "Connecting to pipelineapi.org (pipelineapi.org)|35.237.166.166|:9555... connected.\n",
      "HTTP request sent, awaiting response... 200 \n",
      "Length: 902791782 (861M) [application/octet-stream]\n",
      "Saving to: ‘WFDB_PTB.tar.gz’\n",
      "\n",
      "WFDB_PTB.tar.gz     100%[===================>] 860.97M  22.2MB/s    in 58s     \n",
      "\n",
      "2022-08-24 15:02:55 (14.8 MB/s) - ‘WFDB_PTB.tar.gz’ saved [902791782/902791782]\n",
      "\n",
      "--2022-08-24 15:02:55--  https://pipelineapi.org:9555/api/download/physionettraining/WFDB_PTBXL.tar.gz/\n",
      "Resolving pipelineapi.org (pipelineapi.org)... 35.237.166.166\n",
      "Connecting to pipelineapi.org (pipelineapi.org)|35.237.166.166|:9555... connected.\n",
      "HTTP request sent, awaiting response... 200 \n",
      "Length: 1371121893 (1.3G) [application/octet-stream]\n",
      "Saving to: ‘WFDB_PTBXL.tar.gz’\n",
      "\n",
      "WFDB_PTBXL.tar.gz   100%[===================>]   1.28G  23.3MB/s    in 89s     \n",
      "\n",
      "2022-08-24 15:04:25 (14.7 MB/s) - ‘WFDB_PTBXL.tar.gz’ saved [1371121893/1371121893]\n",
      "\n",
      "--2022-08-24 15:04:25--  https://pipelineapi.org:9555/api/download/physionettraining/WFDB_Ga.tar.gz/\n",
      "Resolving pipelineapi.org (pipelineapi.org)... 35.237.166.166\n",
      "Connecting to pipelineapi.org (pipelineapi.org)|35.237.166.166|:9555... connected.\n",
      "HTTP request sent, awaiting response... 200 \n",
      "Length: 502266323 (479M) [application/octet-stream]\n",
      "Saving to: ‘WFDB_Ga.tar.gz’\n",
      "\n",
      "WFDB_Ga.tar.gz      100%[===================>] 479.00M  18.7MB/s    in 53s     \n",
      "\n",
      "2022-08-24 15:05:18 (9.09 MB/s) - ‘WFDB_Ga.tar.gz’ saved [502266323/502266323]\n",
      "\n",
      "--2022-08-24 15:05:18--  https://pipelineapi.org:9555/api/download/physionettraining/WFDB_ChapmanShaoxing.tar.gz/\n",
      "Resolving pipelineapi.org (pipelineapi.org)... 35.237.166.166\n",
      "Connecting to pipelineapi.org (pipelineapi.org)|35.237.166.166|:9555... connected.\n",
      "HTTP request sent, awaiting response... 200 \n",
      "Length: 566560497 (540M) [application/octet-stream]\n",
      "Saving to: ‘WFDB_ChapmanShaoxing.tar.gz’\n",
      "\n",
      "WFDB_ChapmanShaoxin 100%[===================>] 540.31M  23.2MB/s    in 34s     \n",
      "\n",
      "2022-08-24 15:05:53 (16.0 MB/s) - ‘WFDB_ChapmanShaoxing.tar.gz’ saved [566560497/566560497]\n",
      "\n",
      "--2022-08-24 15:05:53--  https://pipelineapi.org:9555/api/download/physionettraining/WFDB_Ningbo.tar.gz/\n",
      "Resolving pipelineapi.org (pipelineapi.org)... 35.237.166.166\n",
      "Connecting to pipelineapi.org (pipelineapi.org)|35.237.166.166|:9555... connected.\n",
      "HTTP request sent, awaiting response... 200 \n",
      "Length: 1889846085 (1.8G) [application/octet-stream]\n",
      "Saving to: ‘WFDB_Ningbo.tar.gz’\n",
      "\n",
      "WFDB_Ningbo.tar.gz  100%[===================>]   1.76G  23.4MB/s    in 95s     \n",
      "\n",
      "2022-08-24 15:07:28 (19.0 MB/s) - ‘WFDB_Ningbo.tar.gz’ saved [1889846085/1889846085]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# First we need the tar.gz files of each database so let's download them first\n",
    "\n",
    "!wget -O WFDB_CPSC2018.tar.gz \\\n",
    "https://pipelineapi.org:9555/api/download/physionettraining/WFDB_CPSC2018.tar.gz/\n",
    "        \n",
    "!wget -O WFDB_CPSC2018_2.tar.gz \\\n",
    "https://pipelineapi.org:9555/api/download/physionettraining/WFDB_CPSC2018_2.tar.gz/\n",
    "        \n",
    "!wget -O WFDB_StPetersburg.tar.gz \\\n",
    "https://pipelineapi.org:9555/api/download/physionettraining//WFDB_StPetersburg.tar.gz/\n",
    "        \n",
    "!wget -O WFDB_PTB.tar.gz \\\n",
    "https://pipelineapi.org:9555/api/download/physionettraining/WFDB_PTB.tar.gz/\n",
    "        \n",
    "!wget -O WFDB_PTBXL.tar.gz \\\n",
    "https://pipelineapi.org:9555/api/download/physionettraining/WFDB_PTBXL.tar.gz/\n",
    "        \n",
    "!wget -O WFDB_Ga.tar.gz \\\n",
    "https://pipelineapi.org:9555/api/download/physionettraining/WFDB_Ga.tar.gz/\n",
    "        \n",
    "!wget -O WFDB_ChapmanShaoxing.tar.gz \\\n",
    "https://pipelineapi.org:9555/api/download/physionettraining/WFDB_ChapmanShaoxing.tar.gz/\n",
    "        \n",
    "!wget -O WFDB_Ningbo.tar.gz \\\n",
    "https://pipelineapi.org:9555/api/download/physionettraining/WFDB_Ningbo.tar.gz/\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "497a0fdc",
   "metadata": {},
   "source": [
    "Once we have the files, they need to be extracted to the `data` directory which is located in the root of the repository. We might want to extract the files based on the source as follows\n",
    "\n",
    "- CPSC Database and CPSC-Extra Database\n",
    "- St. Petersberg (INCART) Database\n",
    "- PTB and PTB-XL Database\n",
    "- The Georgia 12-lead ECG Challenge (G12EC) Database\n",
    "- Chapman-Shaoxing and Ningbo Database\n",
    "\n",
    "Let's have the data files in such structure. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "11b1e0f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 WFDB_CPSC2018.tar.gz\n",
      "1 WFDB_CPSC2018_2.tar.gz\n",
      "2 WFDB_ChapmanShaoxing.tar.gz\n",
      "3 WFDB_Ga.tar.gz\n",
      "4 WFDB_Ningbo.tar.gz\n",
      "5 WFDB_PTB.tar.gz\n",
      "6 WFDB_PTBXL.tar.gz\n",
      "7 WFDB_StPetersburg.tar.gz\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# All tar.gz files (in the current working directory)\n",
    "curr_path = os.getcwd()\n",
    "targz_files = [file for file in os.listdir(curr_path) if os.path.isfile(os.path.join(curr_path, file)) and file.endswith('tar.gz')]\n",
    "\n",
    "# Let's sort the files\n",
    "targz_files = sorted(targz_files)\n",
    "\n",
    "for i, file in enumerate(targz_files):\n",
    "    print(i, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0560e002",
   "metadata": {},
   "source": [
    "So we want to extract the tar.gz files listed above as\n",
    "\n",
    "* WFDB_CPSC2018.tar.gz + WFDB_CPSC2018_2.tar.gz\n",
    "* WFDB_StPetersburg.tar.gz\n",
    "* WFDB_PTB.tar.gz + WFDB_PTBXL.tar.gz\n",
    "* WFDB_Ga.tar.gz\n",
    "* WFDB_ChapmanShaoxing.tar.gz + WFDB_Ningbo.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "131b041f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('WFDB_CPSC2018.tar.gz', 'WFDB_CPSC2018_2.tar.gz')\n",
      "('WFDB_StPetersburg.tar.gz',)\n",
      "('WFDB_PTB.tar.gz', 'WFDB_PTBXL.tar.gz')\n",
      "('WFDB_Ga.tar.gz',)\n",
      "('WFDB_ChapmanShaoxing.tar.gz', 'WFDB_Ningbo.tar.gz')\n"
     ]
    }
   ],
   "source": [
    "# Let's make the split as tuples of tar.gz files\n",
    "# NB! If the split mentioned above wanted, SORTING is really important!\n",
    "tar_split = [(targz_files[0], targz_files[1]),\n",
    "             (targz_files[7], ),\n",
    "             (targz_files[5], targz_files[6]),\n",
    "             (targz_files[3], ),\n",
    "             (targz_files[2], targz_files[4])]\n",
    "\n",
    "print(*tar_split, sep=\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7f886269",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tarfile\n",
    "\n",
    "# Function to extract files from a given tar to a given directory\n",
    "# Will exclude subdirectories from a given tar and load all the files directly to the given directory\n",
    "def extract_files(tar, directory):\n",
    "    \n",
    "    file = tarfile.open(tar, 'r')\n",
    "    \n",
    "    n_files = 0\n",
    "    for member in file.getmembers():\n",
    "        if member.isreg(): # Skip if the TarInfo is not file\n",
    "            member.name = os.path.basename(member.name) # Reset path\n",
    "            file.extract(member, directory)\n",
    "            n_files += 1\n",
    "    \n",
    "    file.close() \n",
    "    print('- {} files extracted to {}'.format(n_files, directory))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d69e560b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting tar.gz file(s) ('WFDB_CPSC2018.tar.gz', 'WFDB_CPSC2018_2.tar.gz') to the CPSC_CPSC-Extra directory\n",
      "- 13754 files extracted to ../data/physionet_data/CPSC_CPSC-Extra\n",
      "- 6906 files extracted to ../data/physionet_data/CPSC_CPSC-Extra\n",
      "Extracting tar.gz file(s) ('WFDB_StPetersburg.tar.gz',) to the INCART directory\n",
      "- 148 files extracted to ../data/physionet_data/INCART\n",
      "Extracting tar.gz file(s) ('WFDB_PTB.tar.gz', 'WFDB_PTBXL.tar.gz') to the PTB_PTBXL directory\n",
      "- 1032 files extracted to ../data/physionet_data/PTB_PTBXL\n",
      "- 43674 files extracted to ../data/physionet_data/PTB_PTBXL\n",
      "Extracting tar.gz file(s) ('WFDB_Ga.tar.gz',) to the G12EC directory\n",
      "- 20688 files extracted to ../data/physionet_data/G12EC\n",
      "Extracting tar.gz file(s) ('WFDB_ChapmanShaoxing.tar.gz', 'WFDB_Ningbo.tar.gz') to the ChapmanShaoxing_Ningbo directory\n",
      "- 20494 files extracted to ../data/physionet_data/ChapmanShaoxing_Ningbo\n",
      "- 69810 files extracted to ../data/physionet_data/ChapmanShaoxing_Ningbo\n"
     ]
    }
   ],
   "source": [
    "# Path to the physionet_data directory, i.e., save the dataset here\n",
    "data_path = '../data/physionet_data'\n",
    "if not os.path.exists(data_path):\n",
    "       os.makedirs(data_path)\n",
    "\n",
    "# Directories to which extract the data\n",
    "# NB! Gotta be at the same length than 'tar_split'\n",
    "dir_names = ['CPSC_CPSC-Extra', 'INCART', 'PTB_PTBXL', 'G12EC', 'ChapmanShaoxing_Ningbo']\n",
    "\n",
    "# Extracting right files to right subdirectories\n",
    "for tar, directory in zip(tar_split, dir_names):\n",
    "    \n",
    "    print('Extracting tar.gz file(s) {} to the {} directory'.format(tar, directory))\n",
    "    \n",
    "    # Saving path for the specific files\n",
    "    save_tmp = os.path.join(data_path, directory)\n",
    "    # Preparing the directory\n",
    "    if not os.path.exists(save_tmp):\n",
    "        os.makedirs(save_tmp)\n",
    "        \n",
    "    if len(tar) > 1: # More than one database in tuple\n",
    "        for one_tar in tar:\n",
    "            extract_files(one_tar, save_tmp)\n",
    "    else: # Only one database in tuple\n",
    "        extract_files(tar[0], save_tmp)\n",
    "        \n",
    "print('Done!')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "200b35f9",
   "metadata": {},
   "source": [
    "Now we should have total of **176 506** files (if we want to believe the data exploration presented above) in the `physionet_data` directory as one ECG recording consists of a binary MATLAB v4 file and a text file in header format. We might doublecheck that easily:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f6cd287a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total of 176506 files\n"
     ]
    }
   ],
   "source": [
    "total_files = 0\n",
    "for root, dirs, files in os.walk(data_path):\n",
    "    total_files += len(files)\n",
    "    \n",
    "print('Total of {} files'.format(total_files))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec35b911",
   "metadata": {},
   "source": [
    "### Other data sources\n",
    "\n",
    "Wanted data can also be downloaded from other sources when few quidelines are followed:\n",
    "\n",
    "1) When using this repository in training and testing, the model processes ECGs in `MATLAB v4` format (.mat) and header files in `WFDB header format` format (.hea). Header files consist of the describtion of the recording and patient attributes, including *diagnoses*. \n",
    "\n",
    "The following code is used to load the data from MATLAB files:\n",
    "\n",
    "```\n",
    "def load_data(case):\n",
    "    ''' Loading the MATLAB v4 file of ECG recording\n",
    "    '''\n",
    "    x = loadmat(case)\n",
    "    return np.asarray(x['val'], dtype=np.float64)\n",
    "```\n",
    "\n",
    "So there is a column named `val` in which the recording is located. Consider this when loading other MATLAB files.\n",
    "\n",
    "2) Data should be located in the `data` directory. For example, then training and making predictions, the attribute `data_root` is set from where the ECG recordings are loaded.\n",
    "\n",
    "The above code extracts tar.gz files and the chunk consisting of `extract_files(tar, directory)` is generally usable. The function parameters `tar` refers to tar.gz file which needs to be extracted, and `directory` refers to the path in which the file is extracted to. The path is formated as a relative path, e.g. `../data/physionet_data/G12EC`.\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2edd6be1",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Other sources\n",
    "## -------------\n",
    "\n",
    "# tar = 'example.tar.gz'\n",
    "# save_path = '../data/example/'\n",
    "# extract_files(tar, save_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f2cd392",
   "metadata": {},
   "source": [
    "----\n",
    "\n",
    "## 2) Preprocessing data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23022168",
   "metadata": {},
   "source": [
    "All the data can be preprocessed with different transforms with the script `preprocess_data.py`. There are two important attributes to consider:\n",
    "\n",
    "```\n",
    "# Original data location\n",
    "from_directory = os.path.join(os.getcwd(), 'data', 'physionet_data_smoke')\n",
    "\n",
    "# New location for preprocessed data\n",
    "new_directory = os.path.join(os.getcwd(), 'data', 'physionet_preprocessed_smoke')\n",
    "```\n",
    "\n",
    "`from_directory` refers to the directory where the data in the original format is loaded from, such as downloaded Physionet Challenge data. `new_directory`, as it's name suggests, refers to the new location where the directory tree of the original data location is first copied using a function `copy_tree` from the module `distutils.dir_util`. After this, *each directory in the new location* is iterated over and all the ECGs (which should be in MatLab format) are preprocessed with wanted transforms. *An original version of ECG is afterwards deleted and the preprocessed one saved in the directory.*\n",
    "\n",
    "By default there are two transforms used, linear interpolation and BandPass filter:\n",
    "\n",
    "```\n",
    "# ------------------------------\n",
    "# --- PREPROCESS TRANSFORMS ----\n",
    "\n",
    "# - BandPass filter \n",
    "bpf = BandPassFilter(fs = ecg_fs)\n",
    "ecg = bpf(ecg)\n",
    "\n",
    "# - Linear interpolation\n",
    "linear_interp = Linear_interpolation(fs_new = 257, fs_old = fs)\n",
    "ecg = linear_interp(ecg)\n",
    "\n",
    "# ------------------------------\n",
    "# ------------------------------\n",
    "```\n",
    "\n",
    "The preprocessing part **is not mandatory for the repository to work**. But if transforms, such as the two mentioned, are used e.g. during the training phase, that can significantly slow down training. That's why it's recommended to preprocess the data before training using the script mentioned.\n",
    "\n",
    "All the other transforms are set in the script `dataset.py` in `src/dataloader/`, which is run during training. Several transforms are already available in the script `transforms.py` --- from where `Linear_interpolation` and `BandPassFilter` can be found too --- in the same path.\n",
    "\n",
    "### Terminal command\n",
    "\n",
    "To use the script, simply use the following command\n",
    "\n",
    "```\n",
    "python preprocess_data.py\n",
    "```\n",
    "\n",
    "<font color = red>**NOTE!** The preprocessed ECGs will have different names as the original ones so it's important to mind if the preprosessing part is done or not!</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9e0d492",
   "metadata": {},
   "source": [
    "--------\n",
    "\n",
    "## 3) Splitting data for training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f439d39e",
   "metadata": {},
   "source": [
    "All the data splitting is done with the script `prepare_data.py`. The main idea for that script is to split the data into csv files which we can later use in training and testing a model. The csv files will be stored in a directory named after the yaml file used for split in `/data/split_csvs/`.\n",
    "\n",
    "Csv files have the columns `path` (path for an ECG recording), `age` , `gender` and all the diagnoses in SNOMED CT codes used as labels in classification. The main structure of csv files are as follows:\n",
    "\n",
    "\n",
    "| path  | age  | gender  | 10370003  | 111975006 | 164890007 | *other diagnoses...* |\n",
    "| ------------- |-------------|-------------| ------------- |-------------|-------------|-------------|\n",
    "| ./data/A0002.mat | 49.0 | Female | 0 | 0 | 1 | ... |\n",
    "| ./data/A0003.mat | 81.0 | Female | 0 | 1 | 1 | ... |\n",
    "| ./data/A0004.mat | 45.0 |  Male  | 1 | 0 | 0 | ... |\n",
    "| ... | ... |  ...  | ... | ... | ... | ... |\n",
    "\n",
    "\n",
    "The script uses `yaml` files to load the configurations for different splits. They are located in `/configs/data_splitting`. There are two types of yaml files based on the split type (described below), *database-wise (DBwise)* and *stratified*.\n",
    "\n",
    "The yaml file for a <font color = green>**database-wise split**</font> is constructed like one below (`physionet_DBwise_smoke.yaml`):\n",
    "\n",
    "```\n",
    "# STRATIFIED OR DATABASE-WISE SPLIT\n",
    "stratified: False\n",
    "\n",
    "# INITIAL SETTINGS\n",
    "data_dir: './data/physionet_preprocessed_smoke/'\n",
    "save_dir: './data/split_csvs/'\n",
    "\n",
    "```\n",
    "\n",
    "So there are a boolean valued variable `stratified` which makes the difference between the two types of splits, `data_dir` which refers to the location where the data is loaded from, and `save_dir` which refers to the location where the csv files of splits are saved in.\n",
    "\n",
    "The yaml file for a <font color = green>**stratified split**</font> is constructed like one below (`physionet_stratified_smoke.yaml`):\n",
    "\n",
    "```\n",
    "# STRATIFIED OR DATABASE-WISE SPLIT\n",
    "stratified: True\n",
    "\n",
    "# INITIAL SETTINGS\n",
    "data_dir: './data/physionet_preprocessed_smoke/'\n",
    "save_dir: './data/split_csvs/'\n",
    "\n",
    "splits:\n",
    "    - split_1:\n",
    "      train: ['G12EC', 'INCART', 'PTB_PTBXL', 'ChapmanShaoxing_Ningbo']\n",
    "      test: 'CPSC_CPSC-Extra'\n",
    "```\n",
    "\n",
    "where the same variables `stratified`, `data_dir` and `save_dir` are set as with a database-wise yaml but there also is `splits` which is a dictionary of training-testing splits to make different splits of the data at the same time and that way there's no need to run multiple yaml files separately. Train data will be further stratified into training and validation splits which are both saved in csv files. \n",
    "\n",
    "To perform data splits, the class labels are needed to be set in the script. Set the attribute `labels` for that use.\n",
    "\n",
    "<font color = red>**NOTE!**</font> <font color = green> **Here, the** `data_dir` **attribute is set with the assumption that *the data is preprocessed*. If that's not the case, you should use, for example, the original data directory, such as** `./data/physionet_data_smoke/`. The paths for ECGs will be different in the csv files based on the fact if the data is preprocessed or not.</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c76a439",
   "metadata": {},
   "source": [
    "The splitting itself can be done in two ways\n",
    "\n",
    "1) **Database-wise**. Above, we extracted data in the following way \n",
    "\n",
    "   * CPSC Database and CPSC-Extra Database\n",
    "   * St. Petersberg (INCART) Database\n",
    "   * PTB and PTB-XL Database\n",
    "   * The Georgia 12-lead ECG Challenge (G12EC) Database\n",
    "   * Chapman-Shaoxing and Ningbo Database\n",
    "    \n",
    "Now we can use this structure as a baseline for the data split. Simply, the function `read_split_DB(data_directory, save_directory, labels)` uses this structure and creates csvs based on it. The function parameter `data_directory` refers to the location of the data, `save_directory` refers to the location where the csv files will be saved, and `labels` refers to the list of Snomed CT Coded labels which will be used in the classification. Csv files are named according to the directories from which they were created, e.g., a csv file of CPSC Database and CPSC-Extra Database is names as `CPSC_CPSC-Extra.csv`.\n",
    "\n",
    "We can use this structure when creating yaml files for training and testing. But for example if we need to train a model using the first four sources in the list and using only the Chapman-Shaoxing database in testing, we need to create combined yaml files for training phase. In training we only give one csv file for a model to read which ECGs to use. The other csv files, in which there are ECGs from different databases, are made in the notebook [Yaml files of Database-wise Split for Training and Prediction](2_physionet_DBwise_yaml_files.ipynb) when the training and testing csv files are created.\n",
    "\n",
    "2) **Stratified**. The function `read_split_stratified(data_directory, save_directory, labels, train_val_splits)` will do the stratified split. The function parameters are similar to the ones with the function `read_split_DB` but there are also `train_val_splits` which refers to the dictionary of the splits wanted to perform.\n",
    "\n",
    "Stratification is performed by the multilabel cross validator `MultilabelStratifiedShuffleSplit(n_splits, test_size, train_size, random_state)` from `iterative-stratification` package. The script will be using n_splits sized of the length of training dataset (in the yaml file it will be *4* as data is gathered from 'G12EC', 'INCART', 'PTB_PTBXL' and 'ChapmanShaoxing_Ningbo'). *n_splits must always be at least 2!* More information about this and other multilabel cross validators is available in [the GitHub repository of iterative-stratification](https://github.com/trent-b/iterative-stratification)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0399870e",
   "metadata": {},
   "source": [
    "### Terminal commands\n",
    "\n",
    "The terminal command to perform the database-wise data splitting is as\n",
    "\n",
    "```\n",
    "python prepare_data.py physionet_DBwise_smoke.yaml\n",
    "```\n",
    "\n",
    "The terminal command to perform the stratified data splitting is as\n",
    "\n",
    "```\n",
    "python prepare_data.py physionet_stratified_smoke.yaml\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5440b5d2",
   "metadata": {},
   "source": [
    "-------------\n",
    "\n",
    "## Example: Smoke testing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3484469",
   "metadata": {},
   "source": [
    "*All the data files for smoke testing are available in the repository.*\n",
    "\n",
    "First, we want to preprocess the data. We make sure that the script `preprocess_data.py` has the original and new directories set as follows\n",
    "\n",
    "```\n",
    "from_directory = os.path.join(os.getcwd(), 'data', 'physionet_data_smoke')\n",
    "new_directory = os.path.join(os.getcwd(), 'data', 'physionet_preprocessed_smoke')\n",
    "```\n",
    "\n",
    "So the attribute `from_directory` refers to the directory where the original data is located, and the attribute `new_directory` where the preprocessed data is saved. Now we can perform preprocessing with the following command:\n",
    "\n",
    "```\n",
    "python preprocess_data.py\n",
    "```\n",
    "\n",
    "As data is preprocessed, we can move on to the splitting part. The script `prepare_data.py` should have the attribute `label` set as\n",
    "\n",
    "```\n",
    "labels = ['426783006', '426177001', '164934002', '427084000', '164890007', '39732003', '164889003', '59931005', '427393009', '270492004']\n",
    "```\n",
    "\n",
    "So the class label used are the 10 most common class labels in the whole Physionet Challenge dataset (referring to [Exploration of the PhysioNet2021 Data](#exploration_physionet2021_data)). The class labels in more detail are as follows:\n",
    "\n",
    "name | SNOMED CT code | Total number of diagnoses <br>in the whole data\n",
    "-----|----------------|-------------------------------------------\n",
    "sinus rhythm |426783006 | 28971\n",
    "sinus bradycardia| 426177001 | 18918 \n",
    "t wave abnormal| 164934002 | 11716\n",
    "sinus tachycardia |427084000 | 9657 \n",
    "atrial flutter| 164890007 | 8374\n",
    "left axis deviation |39732003 | 7631 \n",
    "atrial fibrillation |164889003 | 5255 \n",
    "t wave inversion| 59931005 | 3989 \n",
    "sinus arrhythmia |427393009 | 3790\n",
    "1st degree av block| 270492004 | 3534 \n",
    "\n",
    "#### Database-wise split\n",
    "\n",
    "The yaml file for the database-wise split is presented above, `physionet_DBwise_smoke.yaml`.\n",
    "\n",
    "Simply, the split is made with the command\n",
    "\n",
    "```\n",
    "python prepare_data.py physionet_DBwise_smoke.yaml\n",
    "```\n",
    "\n",
    "The csv files are then located in `./data/split_csvs/physionet_DBwise_smoke/` as\n",
    "\n",
    "```\n",
    "ChapmanShaoxing_Ningbo.csv\n",
    "CPSC_CPSC-Extra.csv\n",
    "G12EC.csv\n",
    "INCART.csv\n",
    "PTB_PTBXL.csv\n",
    "```\n",
    "\n",
    "#### Stratified split\n",
    "\n",
    "The yaml for the stratified split is presented above, `physionet_stratified_smoke.yaml`. There is one split which is made by running the file.\n",
    "\n",
    "- Train data is from the directories *G12EC, INCART, PTB_PTBXL* and *ChapmanShaoxing_Ningbo*\n",
    "- Test data is from the directory *CPSC_CPSC-Extra*.\n",
    "\n",
    "The command is the following\n",
    "\n",
    "```\n",
    "python prepare_data.py physionet_stratified_smoke.yaml\n",
    "```\n",
    "\n",
    "The csv files are then located in `./data/split_csvs/physionet_stratified_smoke/` and you should now find the following files in the path:\n",
    "\n",
    "```\n",
    "test_split0.csv\n",
    "train_split0_0.csv\n",
    "train_split0_1.csv\n",
    "train_split0_2.csv\n",
    "train_split0_3.csv\n",
    "val_split0_0.csv\n",
    "val_split0_1.csv\n",
    "val_split0_2.csv\n",
    "val_split0_3.csv\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "16f5b46f222e2a3e8d4adbf7141cae37b71ed37616e60735fa5d1164a1bc3ada"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
