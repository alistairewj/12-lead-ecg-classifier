{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "71f7df4c",
   "metadata": {},
   "source": [
    "# Introduction for Data Handling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23f477ab",
   "metadata": {},
   "source": [
    "This notebook contains information about\n",
    "\n",
    "1)  how to download data into the repository (especially the Physionet 2021 data)\n",
    "\n",
    "2)  how to preprocess data if needed\n",
    "\n",
    "3)  the base idea of splitting data into csv files and how to perform it\n",
    "\n",
    "When you have performed possible preprocessing and the data splitting into csv files, you may want to create `yaml` files based on these files for training and testing. To do this, check the notebooks [Yaml files of Database-wise Split for Training and Testing](2_physionet_DBwise_yaml_files.ipynb) and [Yaml files of Stratified Split for Training and Testing](2_physionet_stratified_yaml_files.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8624e730",
   "metadata": {},
   "source": [
    "--------\n",
    "\n",
    "## 1) Downloading data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13bf81a5",
   "metadata": {},
   "source": [
    "### Physionet 2021 data\n",
    "\n",
    "The exploration of the dataset is available in the notebook [Exploration of the PhysioNet2021 Data](exploration_physionet2021_data.ipynb).\n",
    "\n",
    "There are two ways to download the Physionet Challenge 2021 data in `tar.gz` format: \n",
    "\n",
    "1) Download the data manually from [here](https://moody-challenge.physionet.org/2021/) under **Data Access**\n",
    "\n",
    "2) Let this notebook do the job with the following code\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d1d7b0cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2022-12-12 14:46:50--  https://pipelineapi.org:9555/api/download/physionettraining/WFDB_CPSC2018.tar.gz/\n",
      "Resolving pipelineapi.org (pipelineapi.org)... 35.237.166.166\n",
      "Connecting to pipelineapi.org (pipelineapi.org)|35.237.166.166|:9555... connected.\n",
      "HTTP request sent, awaiting response... 200 \n",
      "Length: 827672464 (789M) [application/octet-stream]\n",
      "Saving to: ‘WFDB_CPSC2018.tar.gz’\n",
      "\n",
      "WFDB_CPSC2018.tar.g 100%[===================>] 789.33M  6.53MB/s    in 1m 46s  \n",
      "\n",
      "2022-12-12 14:48:38 (7.45 MB/s) - ‘WFDB_CPSC2018.tar.gz’ saved [827672464/827672464]\n",
      "\n",
      "--2022-12-12 14:48:38--  https://pipelineapi.org:9555/api/download/physionettraining/WFDB_CPSC2018_2.tar.gz/\n",
      "Resolving pipelineapi.org (pipelineapi.org)... 35.237.166.166\n",
      "Connecting to pipelineapi.org (pipelineapi.org)|35.237.166.166|:9555... connected.\n",
      "HTTP request sent, awaiting response... 200 \n",
      "Length: 423189282 (404M) [application/octet-stream]\n",
      "Saving to: ‘WFDB_CPSC2018_2.tar.gz’\n",
      "\n",
      "WFDB_CPSC2018_2.tar 100%[===================>] 403.58M  6.58MB/s    in 57s     \n",
      "\n",
      "2022-12-12 14:49:35 (7.10 MB/s) - ‘WFDB_CPSC2018_2.tar.gz’ saved [423189282/423189282]\n",
      "\n",
      "--2022-12-12 14:49:35--  https://pipelineapi.org:9555/api/download/physionettraining//WFDB_StPetersburg.tar.gz/\n",
      "Resolving pipelineapi.org (pipelineapi.org)... 35.237.166.166\n",
      "Connecting to pipelineapi.org (pipelineapi.org)|35.237.166.166|:9555... connected.\n",
      "HTTP request sent, awaiting response... 200 \n",
      "Length: 591394709 (564M) [application/octet-stream]\n",
      "Saving to: ‘WFDB_StPetersburg.tar.gz’\n",
      "\n",
      "WFDB_StPetersburg.t 100%[===================>] 564.00M  7.37MB/s    in 47s     \n",
      "\n",
      "2022-12-12 14:50:22 (12.1 MB/s) - ‘WFDB_StPetersburg.tar.gz’ saved [591394709/591394709]\n",
      "\n",
      "--2022-12-12 14:50:22--  https://pipelineapi.org:9555/api/download/physionettraining/WFDB_PTB.tar.gz/\n",
      "Resolving pipelineapi.org (pipelineapi.org)... 35.237.166.166\n",
      "Connecting to pipelineapi.org (pipelineapi.org)|35.237.166.166|:9555... connected.\n",
      "HTTP request sent, awaiting response... 200 \n",
      "Length: 902791782 (861M) [application/octet-stream]\n",
      "Saving to: ‘WFDB_PTB.tar.gz’\n",
      "\n",
      "WFDB_PTB.tar.gz     100%[===================>] 860.97M  20.6MB/s    in 82s     \n",
      "\n",
      "2022-12-12 14:51:45 (10.5 MB/s) - ‘WFDB_PTB.tar.gz’ saved [902791782/902791782]\n",
      "\n",
      "--2022-12-12 14:51:45--  https://pipelineapi.org:9555/api/download/physionettraining/WFDB_PTBXL.tar.gz/\n",
      "Resolving pipelineapi.org (pipelineapi.org)... 35.237.166.166\n",
      "Connecting to pipelineapi.org (pipelineapi.org)|35.237.166.166|:9555... connected.\n",
      "HTTP request sent, awaiting response... 200 \n",
      "Length: 1371121893 (1.3G) [application/octet-stream]\n",
      "Saving to: ‘WFDB_PTBXL.tar.gz’\n",
      "\n",
      "WFDB_PTBXL.tar.gz   100%[===================>]   1.28G  9.71MB/s    in 2m 30s  \n",
      "\n",
      "2022-12-12 14:54:16 (8.72 MB/s) - ‘WFDB_PTBXL.tar.gz’ saved [1371121893/1371121893]\n",
      "\n",
      "--2022-12-12 14:54:16--  https://pipelineapi.org:9555/api/download/physionettraining/WFDB_Ga.tar.gz/\n",
      "Resolving pipelineapi.org (pipelineapi.org)... 35.237.166.166\n",
      "Connecting to pipelineapi.org (pipelineapi.org)|35.237.166.166|:9555... connected.\n",
      "HTTP request sent, awaiting response... 200 \n",
      "Length: 502266323 (479M) [application/octet-stream]\n",
      "Saving to: ‘WFDB_Ga.tar.gz’\n",
      "\n",
      "WFDB_Ga.tar.gz      100%[===================>] 479.00M  7.07MB/s    in 48s     \n",
      "\n",
      "2022-12-12 14:55:04 (10.0 MB/s) - ‘WFDB_Ga.tar.gz’ saved [502266323/502266323]\n",
      "\n",
      "--2022-12-12 14:55:04--  https://pipelineapi.org:9555/api/download/physionettraining/WFDB_ChapmanShaoxing.tar.gz/\n",
      "Resolving pipelineapi.org (pipelineapi.org)... 35.237.166.166\n",
      "Connecting to pipelineapi.org (pipelineapi.org)|35.237.166.166|:9555... connected.\n",
      "HTTP request sent, awaiting response... 200 \n",
      "Length: 566560497 (540M) [application/octet-stream]\n",
      "Saving to: ‘WFDB_ChapmanShaoxing.tar.gz’\n",
      "\n",
      "WFDB_ChapmanShaoxin 100%[===================>] 540.31M  5.64MB/s    in 44s     \n",
      "\n",
      "2022-12-12 14:55:49 (12.4 MB/s) - ‘WFDB_ChapmanShaoxing.tar.gz’ saved [566560497/566560497]\n",
      "\n",
      "--2022-12-12 14:55:49--  https://pipelineapi.org:9555/api/download/physionettraining/WFDB_Ningbo.tar.gz/\n",
      "Resolving pipelineapi.org (pipelineapi.org)... 35.237.166.166\n",
      "Connecting to pipelineapi.org (pipelineapi.org)|35.237.166.166|:9555... connected.\n",
      "HTTP request sent, awaiting response... 200 \n",
      "Length: 1889846085 (1.8G) [application/octet-stream]\n",
      "Saving to: ‘WFDB_Ningbo.tar.gz’\n",
      "\n",
      "WFDB_Ningbo.tar.gz  100%[===================>]   1.76G  8.12MB/s    in 3m 24s  \n",
      "\n",
      "2022-12-12 14:59:14 (8.83 MB/s) - ‘WFDB_Ningbo.tar.gz’ saved [1889846085/1889846085]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# First we need the tar.gz files of each database so let's download them\n",
    "\n",
    "!wget -O WFDB_CPSC2018.tar.gz \\\n",
    "https://pipelineapi.org:9555/api/download/physionettraining/WFDB_CPSC2018.tar.gz/\n",
    "        \n",
    "!wget -O WFDB_CPSC2018_2.tar.gz \\\n",
    "https://pipelineapi.org:9555/api/download/physionettraining/WFDB_CPSC2018_2.tar.gz/\n",
    "        \n",
    "!wget -O WFDB_StPetersburg.tar.gz \\\n",
    "https://pipelineapi.org:9555/api/download/physionettraining//WFDB_StPetersburg.tar.gz/\n",
    "        \n",
    "!wget -O WFDB_PTB.tar.gz \\\n",
    "https://pipelineapi.org:9555/api/download/physionettraining/WFDB_PTB.tar.gz/\n",
    "        \n",
    "!wget -O WFDB_PTBXL.tar.gz \\\n",
    "https://pipelineapi.org:9555/api/download/physionettraining/WFDB_PTBXL.tar.gz/\n",
    "        \n",
    "!wget -O WFDB_Ga.tar.gz \\\n",
    "https://pipelineapi.org:9555/api/download/physionettraining/WFDB_Ga.tar.gz/\n",
    "        \n",
    "!wget -O WFDB_ChapmanShaoxing.tar.gz \\\n",
    "https://pipelineapi.org:9555/api/download/physionettraining/WFDB_ChapmanShaoxing.tar.gz/\n",
    "        \n",
    "!wget -O WFDB_Ningbo.tar.gz \\\n",
    "https://pipelineapi.org:9555/api/download/physionettraining/WFDB_Ningbo.tar.gz/\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "497a0fdc",
   "metadata": {},
   "source": [
    "Once the `tar.gz` files are downloaded, they need to be extracted to the `data` directory which is located in the root of the repository. The files may be needed to be extracted based on the source database as follows:\n",
    "\n",
    "- CPSC Database and CPSC-Extra Database\n",
    "- St. Petersberg (INCART) Database\n",
    "- PTB and PTB-XL Database\n",
    "- The Georgia 12-lead ECG Challenge (G12EC) Database\n",
    "- Chapman-Shaoxing and Ningbo Database\n",
    "\n",
    "Let's first get the names of the `tar.gz` files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "11b1e0f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 WFDB_CPSC2018.tar.gz\n",
      "1 WFDB_CPSC2018_2.tar.gz\n",
      "2 WFDB_ChapmanShaoxing.tar.gz\n",
      "3 WFDB_Ga.tar.gz\n",
      "4 WFDB_Ningbo.tar.gz\n",
      "5 WFDB_PTB.tar.gz\n",
      "6 WFDB_PTBXL.tar.gz\n",
      "7 WFDB_StPetersburg.tar.gz\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# All tar.gz files (in the current working directory)\n",
    "curr_path = os.getcwd()\n",
    "targz_files = [file for file in os.listdir(curr_path) if os.path.isfile(os.path.join(curr_path, file)) and file.endswith('tar.gz')]\n",
    "\n",
    "# Let's sort the files\n",
    "targz_files = sorted(targz_files)\n",
    "\n",
    "for i, file in enumerate(targz_files):\n",
    "    print(i, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0560e002",
   "metadata": {},
   "source": [
    "So the `tar.gz` files listed above will be extracted as follows:\n",
    "\n",
    "* WFDB_CPSC2018.tar.gz + WFDB_CPSC2018_2.tar.gz\n",
    "* WFDB_StPetersburg.tar.gz\n",
    "* WFDB_PTB.tar.gz + WFDB_PTBXL.tar.gz\n",
    "* WFDB_Ga.tar.gz\n",
    "* WFDB_ChapmanShaoxing.tar.gz + WFDB_Ningbo.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "131b041f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('WFDB_CPSC2018.tar.gz', 'WFDB_CPSC2018_2.tar.gz')\n",
      "('WFDB_StPetersburg.tar.gz',)\n",
      "('WFDB_PTB.tar.gz', 'WFDB_PTBXL.tar.gz')\n",
      "('WFDB_Ga.tar.gz',)\n",
      "('WFDB_ChapmanShaoxing.tar.gz', 'WFDB_Ningbo.tar.gz')\n"
     ]
    }
   ],
   "source": [
    "# Let's make the split as tuples of tar.gz files\n",
    "# NB! If the split mentioned above wanted, SORTING is really important!\n",
    "tar_split = [(targz_files[0], targz_files[1]),\n",
    "             (targz_files[7], ),\n",
    "             (targz_files[5], targz_files[6]),\n",
    "             (targz_files[3], ),\n",
    "             (targz_files[2], targz_files[4])]\n",
    "\n",
    "print(*tar_split, sep=\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7f886269",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tarfile\n",
    "\n",
    "# Function to extract files from a given tar to a given directory\n",
    "# Will exclude subdirectories from a given tar and load all the files directly to the given directory\n",
    "def extract_files(tar, directory):\n",
    "    \n",
    "    file = tarfile.open(tar, 'r')\n",
    "    \n",
    "    n_files = 0\n",
    "    for member in file.getmembers():\n",
    "        if member.isreg(): # Skip if the TarInfo is not file\n",
    "            member.name = os.path.basename(member.name) # Reset path\n",
    "            file.extract(member, directory)\n",
    "            n_files += 1\n",
    "    \n",
    "    file.close() \n",
    "    print('- {} files extracted to {}'.format(n_files, directory))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d69e560b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting tar.gz file(s) ('WFDB_CPSC2018.tar.gz', 'WFDB_CPSC2018_2.tar.gz') to the CPSC_CPSC-Extra directory\n",
      "- 13754 files extracted to /home/tuhlei/digital_health_tech_files/testing/12-lead-ecg-classifier/data/physionet_data/CPSC_CPSC-Extra\n",
      "- 6906 files extracted to /home/tuhlei/digital_health_tech_files/testing/12-lead-ecg-classifier/data/physionet_data/CPSC_CPSC-Extra\n",
      "Extracting tar.gz file(s) ('WFDB_StPetersburg.tar.gz',) to the INCART directory\n",
      "- 148 files extracted to /home/tuhlei/digital_health_tech_files/testing/12-lead-ecg-classifier/data/physionet_data/INCART\n",
      "Extracting tar.gz file(s) ('WFDB_PTB.tar.gz', 'WFDB_PTBXL.tar.gz') to the PTB_PTBXL directory\n",
      "- 1032 files extracted to /home/tuhlei/digital_health_tech_files/testing/12-lead-ecg-classifier/data/physionet_data/PTB_PTBXL\n",
      "- 43674 files extracted to /home/tuhlei/digital_health_tech_files/testing/12-lead-ecg-classifier/data/physionet_data/PTB_PTBXL\n",
      "Extracting tar.gz file(s) ('WFDB_Ga.tar.gz',) to the G12EC directory\n",
      "- 20688 files extracted to /home/tuhlei/digital_health_tech_files/testing/12-lead-ecg-classifier/data/physionet_data/G12EC\n",
      "Extracting tar.gz file(s) ('WFDB_ChapmanShaoxing.tar.gz', 'WFDB_Ningbo.tar.gz') to the ChapmanShaoxing_Ningbo directory\n",
      "- 20494 files extracted to /home/tuhlei/digital_health_tech_files/testing/12-lead-ecg-classifier/data/physionet_data/ChapmanShaoxing_Ningbo\n",
      "- 69810 files extracted to /home/tuhlei/digital_health_tech_files/testing/12-lead-ecg-classifier/data/physionet_data/ChapmanShaoxing_Ningbo\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "# Absolute path of this file\n",
    "abs_path = Path(os.path.abspath(''))\n",
    "\n",
    "# Path to the physionet_data directory, i.e., save the dataset here\n",
    "data_path = os.path.join(abs_path.parent.absolute(), 'data', 'physionet_data')\n",
    "\n",
    "if not os.path.exists(data_path):\n",
    "    os.makedirs(data_path)\n",
    "\n",
    "# Directories to which extract the data\n",
    "# NB! Gotta be at the same length than 'tar_split'\n",
    "dir_names = ['CPSC_CPSC-Extra', 'INCART', 'PTB_PTBXL', 'G12EC', 'ChapmanShaoxing_Ningbo']\n",
    "\n",
    "# Extracting right files to right subdirectories\n",
    "for tar, directory in zip(tar_split, dir_names):\n",
    "    \n",
    "    print('Extracting tar.gz file(s) {} to the {} directory'.format(tar, directory))\n",
    "    \n",
    "    # Saving path for the specific files\n",
    "    save_tmp = os.path.join(data_path, directory)\n",
    "    # Preparing the directory\n",
    "    if not os.path.exists(save_tmp):\n",
    "        os.makedirs(save_tmp)\n",
    "        \n",
    "    if len(tar) > 1: # More than one database in tuple\n",
    "        for one_tar in tar:\n",
    "            extract_files(one_tar, save_tmp)\n",
    "    else: # Only one database in tuple\n",
    "        extract_files(tar[0], save_tmp)\n",
    "        \n",
    "print('Done!')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "200b35f9",
   "metadata": {},
   "source": [
    "Now total of **176 506** files (if we want to believe the data exploration presented above) should be located in the `physionet_data` directory as one ECG recording consists of a binary MATLAB v4 file and a text file in header format. For a double check, the number of files can be easily counted as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f6cd287a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total of 176506 files\n"
     ]
    }
   ],
   "source": [
    "total_files = 0\n",
    "for root, dirs, files in os.walk(data_path):\n",
    "    total_files += len(files)\n",
    "    \n",
    "print('Total of {} files'.format(total_files))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec35b911",
   "metadata": {},
   "source": [
    "### Other data sources\n",
    "\n",
    "Wanted data can also be downloaded from other sources when few quidelines are followed:\n",
    "\n",
    "1) When using this repository in training and testing, the model processes ECGs in `MATLAB v4` format (.mat) and header files in `WFDB header format` format (.hea). Header files consist of the describtion of the recording and patient attributes, including *diagnoses*. \n",
    "\n",
    "The following code is used to load the data from MATLAB files:\n",
    "\n",
    "```\n",
    "def load_data(case):\n",
    "    ''' Loading the MATLAB v4 file of ECG recording\n",
    "    '''\n",
    "    x = loadmat(case)\n",
    "    return np.asarray(x['val'], dtype=np.float64)\n",
    "```\n",
    "\n",
    "So there is a column named `val` in which the recording is located. This should be considered when loading other MATLAB files.\n",
    "\n",
    "2) Data should be located in the `data` directory. For example, when training and making predictions, the `data_root` attribute is set in `train_model.py` and `test_model.py` scripts to  indicate the path where the ECG recordings are loaded from.\n",
    "\n",
    "The above code extracts tar.gz files and the chunk consisting of `extract_files(tar, directory)` is generally usable. The function parameters `tar` refers to tar.gz file which needs to be extracted, and `save_path` refers to the path in which the file is extracted to. The path is formatted as an absolute path.\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2edd6be1",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Other sources\n",
    "## -------------\n",
    "\n",
    "# tar = 'example.tar.gz'\n",
    "# save_path = os.path.join(abs_path.parent.absolute(), 'data', 'example')\n",
    "# extract_files(tar, save_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f2cd392",
   "metadata": {},
   "source": [
    "----\n",
    "\n",
    "## 2) Preprocessing data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23022168",
   "metadata": {},
   "source": [
    "All the data can be preprocessed with different transforms with the `preprocess_data.py` script. There are two important attributes to consider:\n",
    "\n",
    "```\n",
    "# Original data location\n",
    "from_directory = os.path.join(os.getcwd(), 'data', 'physionet_data_smoke')\n",
    "\n",
    "# New location for preprocessed data\n",
    "new_directory = os.path.join(os.getcwd(), 'data', 'physionet_preprocessed_smoke')\n",
    "```\n",
    "\n",
    "`from_directory` refers to the directory where the data in the original format is loaded from, such as the downloaded Physionet Challenge 2021 data. `new_directory` refers to the new location where the directory tree of the original data location is first copied using a function `copy_tree` from the module `distutils.dir_util`. After this, *each directory in the new location* is iterated over and all the ECGs (which should be in a MATLAB format) are preprocessed with wanted transforms. *An original version of ECG is afterwards deleted and the preprocessed one saved in the directory.*\n",
    "\n",
    "By default there are two transforms used, a band-pass filter and linear interpolation:\n",
    "\n",
    "```\n",
    "# ------------------------------\n",
    "# --- PREPROCESS TRANSFORMS ----\n",
    "\n",
    "# - BandPass filter \n",
    "bpf = BandPassFilter(fs = ecg_fs)\n",
    "ecg = bpf(ecg)\n",
    "\n",
    "# - Linear interpolation\n",
    "linear_interp = Linear_interpolation(fs_new = 257, fs_old = ecg_fs)\n",
    "ecg = linear_interp(ecg)\n",
    "\n",
    "# ------------------------------\n",
    "# ------------------------------\n",
    "```\n",
    "\n",
    "The preprocessing part **is not mandatory for the repository to work**, but if transforms, such as the two mentioned, are used e.g. during the training phase, that can significantly slow down training. That's why it's recommended to preprocess the data before training using the script mentioned.\n",
    "\n",
    "All the other transforms are set in the `dataset.py` script in `src/dataloader/`, which is run during training. Several transforms are already available in the script `transforms.py` --- from where `Linear_interpolation` and `BandPassFilter` can be found too --- in the same path.\n",
    "\n",
    "### Terminal command\n",
    "\n",
    "To use the script, simply use the following command\n",
    "\n",
    "```\n",
    "python preprocess_data.py\n",
    "```\n",
    "\n",
    "<font color = red>**NOTE!** The preprocessed ECGs will have different names as the original ones so it's important to mind if the preprosessing part is done or not!</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9e0d492",
   "metadata": {},
   "source": [
    "--------\n",
    "\n",
    "## 3) Splitting data for training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f439d39e",
   "metadata": {},
   "source": [
    "All the data splitting is done with the `create_data_split_csvs.py` script. The main idea for that script is to split the data into csv files which can be later used in training and testing.\n",
    "\n",
    "Csv files have the columns `path` (path for a spesific ECG recording), `age`, `gender` and all the diagnoses in SNOMED CT codes used as labels in classification. A value of 1 means that the patient has the disease. The main structure of csv files is as follows:\n",
    "\n",
    "\n",
    "| path  | age  | gender  | 10370003  | 111975006 | 164890007 | *other diagnoses...* |\n",
    "| ------------- |-------------|-------------| ------------- |-------------|-------------|-------------|\n",
    "| ./data/A0002.mat | 49.0 | Female | 0 | 0 | 1 | ... |\n",
    "| ./data/A0003.mat | 81.0 | Female | 0 | 1 | 1 | ... |\n",
    "| ./data/A0004.mat | 45.0 |  Male  | 1 | 0 | 0 | ... |\n",
    "| ... | ... |  ...  | ... | ... | ... | ... |\n",
    "\n",
    "\n",
    "The script includes several attributes which need to be considered in the main block before running: \n",
    "\n",
    "1) The split itself is needed to be spesified using the `stratified` attribute which is a boolean. If the attribute is set `True`, the script performs stratified data split and respectively, if `False`, the database-wise split is performed. \n",
    "\n",
    "2) The `data_dir` attribute should be set to point to the right data directory where the data is loaded from. By default it's set to load the data from the `physionet_preprocessed_smoke` directory, which is the subdirectory of the `data` directory. \n",
    "\n",
    "3) The `csv_dir` attribute should be set to point to the wanted directory where the created csv files will be saved. By default it's set to save the csv files to the `physionet_stratified_smoke` directory which is found from `../data/split_csvs`.\n",
    "\n",
    "4) The class labels are needed to be set with the `labels` attribute in the script. By default the labels are \n",
    "`'426783006', '426177001', '164934002', '427084000', '164890007', '39732003', '164889003', '59931005', '427393009' and '270492004'`, which are ten most common labels found in the [Exploration of the PhysioNet 2021 Data](./exploration_physionet2021_data.ipynb). The numbers of each diagnosis are represented below:\n",
    "\n",
    "name | SNOMED CT code | Total number of diagnoses<br>in the whole data\n",
    "-----|----------------|-------------------------------------------\n",
    "sinus rhythm |426783006 | 28971\n",
    "sinus bradycardia| 426177001 | 18918 \n",
    "t wave abnormal| 164934002 | 11716\n",
    "sinus tachycardia |427084000 | 9657 \n",
    "atrial flutter| 164890007 | 8374\n",
    "left axis deviation |39732003 | 7631 \n",
    "atrial fibrillation |164889003 | 5255 \n",
    "t wave inversion| 59931005 | 3989 \n",
    "sinus arrhythmia |427393009 | 3790\n",
    "1st degree av block| 270492004 | 3534 \n",
    "\n",
    "<font color = red>**NOTE!**</font> <font color = green> **Here, the** `data_dir` **attribute is set with the assumption that *the data is preprocessed*. If that's not the case, you should use, for example, the original data directory, such as the** `physionet_data_smoke` **directory.** The paths for ECGs will be different in the csv files depending on whether preprocessing has been used or not.</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c76a439",
   "metadata": {},
   "source": [
    "\n",
    "The splitting itself can be done in two ways\n",
    "\n",
    "1) **Database-wise**. Above, the data was extracted in the following way \n",
    "\n",
    "   * CPSC Database and CPSC-Extra Database\n",
    "   * St. Petersberg (INCART) Database\n",
    "   * PTB and PTB-XL Database\n",
    "   * The Georgia 12-lead ECG Challenge (G12EC) Database\n",
    "   * Chapman-Shaoxing and Ningbo Database\n",
    "   \n",
    "   This structure can be used as a baseline for the data split. Simply, the function `dbwise_csvs(data_directory, save_directory, labels)` uses this structure and creates csvs based on it. The `data_directory` parameter refers to the location of the data, `save_directory` refers to the location where the csv files will be saved, and `labels` refers to the list of Snomed CT Coded labels which will be used in classification. Csv files are named according to the directories from which they were created, e.g., a csv file of CPSC Database and CPSC-Extra Database is names as `CPSC_CPSC-Extra.csv`.\n",
    "\n",
    "   We can use this structure when creating yaml files for training and testing. But for example if we need to train a model using the first four sources in the list and using only the Chapman-Shaoxing and Ningbo database in testing, we need to create combined yaml files for training phase. In training we only give one csv file for a model to read which ECGs to use. The other csv files, in which there are ECGs from different databases, are made in the notebook [Yaml files of Database-wise Split for Training and Prediction](2_physionet_DBwise_yaml_files.ipynb) when the training and testing csv files are created.\n",
    "\n",
    "2) **Stratified**. The function `stratified_csvs(data_directory, save_directory, labels, train_test_splits)` will perform the stratified split. The parameters are similar to the ones with the function `dbwise_csvs` but there is also the `train_test_splits` parameter which refers to the dictionary of train-test splits. The dictionary is a nested dictionary, i.e. a collection of dictionaries, where the internal directories refer to spesific train-test splits. For example, by default there's one train-test split set in the `train_test_splits` dictionary as follows:\n",
    "\n",
    "   ```\n",
    "   train_test_splits = {\n",
    "   'split_1': {    \n",
    "         'train': ['G12EC', 'INCART', 'PTB_PTBXL', 'ChapmanShaoxing_Ningbo'],\n",
    "         'test': 'CPSC_CPSC-Extra'\n",
    "      }\n",
    "   }\n",
    "   ```\n",
    "   where `split_1` is simply a name for this particular split, and it has keys `train` and `test` to initialize which databases are seen as training data and which ones as test data. Training data is further divided into training and validation sets. Names (e.g. `split_1`) are used to name the csv files. \n",
    "\n",
    "   Stratification itself is performed by the multilabel cross validator `MultilabelStratifiedShuffleSplit(n_splits, test_size, train_size, random_state)` from `iterative-stratification` package. The script will be using n_splits sized of the length of training dataset (in the yaml file it will be *4* as data is gathered from 'G12EC', 'INCART', 'PTB_PTBXL' and 'ChapmanShaoxing_Ningbo'). *n_splits must always be at least 2!* More information about this and other multilabel cross validators is available in [the GitHub repository of iterative-stratification](https://github.com/trent-b/iterative-stratification)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0399870e",
   "metadata": {},
   "source": [
    "### Terminal commands\n",
    "\n",
    "After initializing the needed attributes, the terminal command to perform wanted data split is the following one:\n",
    "\n",
    "```\n",
    "python create_data_split_csvs.py\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5440b5d2",
   "metadata": {},
   "source": [
    "-------------\n",
    "\n",
    "## Example: Smoke testing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3484469",
   "metadata": {},
   "source": [
    "*All the data files for smoke testing are available in the repository.*\n",
    "\n",
    "First, we want to **preprocess the data**. We make sure that the `preprocess_data.py` script has the original and new directories set as follows\n",
    "\n",
    "```\n",
    "from_directory = os.path.join(os.getcwd(), 'data', 'physionet_data_smoke')\n",
    "new_directory = os.path.join(os.getcwd(), 'data', 'physionet_preprocessed_smoke')\n",
    "```\n",
    "\n",
    "The `from_directory` attribute refers to the directory where the original data is located, and the `new_directory` attribute where the preprocessed data is saved. Now preprocessing is performed with the following command:\n",
    "\n",
    "```\n",
    "python preprocess_data.py\n",
    "```\n",
    "\n",
    "When data is preprocessed, we can move on to **split the data into csv files**. Remember to check that the attributes are set as below before running the following command:\n",
    "\n",
    "```\n",
    "python create_data_split_csvs.py\n",
    "```\n",
    "\n",
    "####  1) Database-wise split\n",
    "\n",
    "The `create_data_split_csvs.py` script should have the following attributes set **before the `if-else` statement** as follows:\n",
    "\n",
    "```\n",
    "stratified = False\n",
    "data_dir =  'physionet_preprocessed_smoke'\n",
    "csv_dir =  'physionet_DBwise_smoke'\n",
    "labels = ['426783006', '426177001', '164934002', '427084000', '164890007', '39732003', '164889003', '59931005', '427393009', '270492004']\n",
    "```\n",
    "\n",
    "The csv files are saved in `./data/split_csvs/physionet_DBwise_smoke/` where you will find the following files:\n",
    "\n",
    "```\n",
    "ChapmanShaoxing_Ningbo.csv\n",
    "CPSC_CPSC-Extra.csv\n",
    "G12EC.csv\n",
    "INCART.csv\n",
    "PTB_PTBXL.csv\n",
    "```\n",
    "\n",
    "#### 2) Stratified split\n",
    "\n",
    "Stratified data split is performed using dictionary of dictionaries where the wanted train-test splits are set. There is one split which is made by running the file.\n",
    "\n",
    "- Train data is from the directories *G12EC, INCART, PTB_PTBXL* and *ChapmanShaoxing_Ningbo*\n",
    "- Test data is from the directory *CPSC_CPSC-Extra*.\n",
    "\n",
    "The following attributes set before the `if-else` statement as follows:\n",
    "\n",
    "```\n",
    "stratified = True\n",
    "data_dir =  'physionet_preprocessed_smoke'\n",
    "csv_dir =  'physionet_stratified_smoke'\n",
    "labels = ['426783006', '426177001', '164934002', '427084000', '164890007', '39732003', '164889003', '59931005', '427393009', '270492004']\n",
    "```\n",
    "\n",
    "And so specify, which databases are used as training set and which one(s) as testing set, the `train_test_splits` attribute should be set **in the if block**.\n",
    "\n",
    "```\n",
    "train_test_splits = {\n",
    "    'split_1': {    \n",
    "        'train': ['G12EC', 'INCART', 'PTB_PTBXL', 'ChapmanShaoxing_Ningbo'],\n",
    "        'test': 'CPSC_CPSC-Extra'\n",
    "    }\n",
    "}\n",
    "```\n",
    "\n",
    "The csv files are saved in `./data/split_csvs/physionet_stratified_smoke/` where you will find the following files:\n",
    "\n",
    "```\n",
    "test_split1.csv\n",
    "train_split_1_1.csv\n",
    "train_split_1_2.csv\n",
    "train_split_1_3.csv\n",
    "train_split_1_4.csv\n",
    "val_split_1_1.csv\n",
    "val_split_1_2.csv\n",
    "val_split_1_3.csv\n",
    "val_split_1_4.csv\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "vscode": {
   "interpreter": {
    "hash": "46963f0590c8313baefd57cb1336ee0094ee3a6da0b1eb974571013be2a14d92"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
