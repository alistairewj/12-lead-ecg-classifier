{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7999d565",
   "metadata": {},
   "source": [
    "# <font color = teal> Yaml files of database-wise split for training and testing </font>\n",
    "\n",
    "With this notebook, you can create the yaml files needed in training and testing with a data split which is made **database-wise**. The so-called \"original data split\" should be first made with the script `create_data_split_csvs.py`. This ensures that there are already one csv file for each database. The combined csv files are created below. More detailed information about this is available in the notebook [Introduction to data handling](1_introduction_data_handling.ipynb). \n",
    "\n",
    "------\n",
    "\n",
    "Note that the hyperparameters considering training and testing are set in the yaml files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8d351bfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Parameters for the yaml files -------------\n",
    "# Training parameters\n",
    "batch_size = 10\n",
    "num_workers = 0\n",
    "epochs = 1\n",
    "lr = 0.003\n",
    "weight_decay = 0.00001\n",
    "\n",
    "# Device configurations\n",
    "device_count = 1\n",
    "\n",
    "# -----------\n",
    "# Decision threshold for predictions\n",
    "threshold = 0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85b2322c",
   "metadata": {},
   "source": [
    "Examples of the training and the testing yaml files are provided below.\n",
    "\n",
    "**Yaml file for training a model**\n",
    "```\n",
    "# INITIAL SETTINGS\n",
    "train_file: train_split_1_1.csv\n",
    "val_file: val_split_1_1.csv\n",
    "\n",
    "# TRAINING SETTINGS\n",
    "batch_size: 10\n",
    "num_workers: 0\n",
    "epochs: 1\n",
    "lr: 0.003000\n",
    "weight_decay: 0.000010\n",
    "\n",
    "# VALIDATION SETTINGS\n",
    "threshold = 0.5\n",
    "\n",
    "# DEVICE CONFIGS\n",
    "device_count: 1\n",
    "\n",
    "```\n",
    "\n",
    "**Yaml file for testing a model**\n",
    "\n",
    "```\n",
    "# INITIAL SETTINGS\n",
    "test_file: test_split_1.csv\n",
    "model: split_1_1.pth\n",
    "\n",
    "# TESTING SETTINGS\n",
    "threshold: 0.500000\n",
    "\n",
    "# DEVICE CONFIGS\n",
    "device_count: 1\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d2f21607",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# Absolute path of this file\n",
    "abs_path = Path(os.path.abspath(''))\n",
    "\n",
    "# PARAMETERS TO CREATE DBWISE YAML FILES  \n",
    "# ----------------------------------------\n",
    "\n",
    "# From where to load the csv files of \"original data split\"\n",
    "# Note that this is the saving location for combined csv files too\n",
    "csv_path = os.path.join(abs_path.parent.absolute(), 'data', 'split_csvs', 'physionet_DBwise_smoke')\n",
    "\n",
    "# Where to save the training yaml files\n",
    "train_yaml_save_path = os.path.join(abs_path.parent.absolute(), 'configs', 'training', 'train_DBwise_smoke')\n",
    "\n",
    "# Where to save the testing yaml files\n",
    "test_yaml_save_path = os.path.join(abs_path.parent.absolute(), 'configs', 'predicting', 'predict_DBwise_smoke')\n",
    "\n",
    "# The files which need to be split into training and validation data\n",
    "# We have 5 different databases so 5 different train/val split sets\n",
    "training_data = [['PTB_PTBXL.csv', 'INCART.csv', 'G12EC.csv', 'ChapmanShaoxing_Ningbo.csv']]\n",
    "\n",
    "# !! All the other splits -> just add to the list above if wanted\n",
    "# ['PTB_PTBXL.csv', 'INCART.csv', 'G12EC.csv', 'CPSC_CPSC-Extra.csv'],\n",
    "# ['PTB_PTBXL.csv', 'INCART.csv', 'CPSC_CPSC-Extra.csv', 'ChapmanShaoxing_Ningbo.csv'],\n",
    "# ['PTB_PTBXL.csv', 'CPSC_CPSC-Extra.csv', 'G12EC.csv', 'ChapmanShaoxing_Ningbo.csv'],\n",
    "# ['INCART.csv', 'CPSC_CPSC-Extra.csv', 'G12EC.csv','ChapmanShaoxing_Ningbo.csv']\n",
    "\n",
    "# Name for yaml files given as a string\n",
    "# names will be formed as <name><index>.yaml\n",
    "name = 'split_'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ced2ffe0",
   "metadata": {},
   "source": [
    "First, the csv files, from which the yaml files are created, need to be found. They should be located in `/data/split_csvs/`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f005f047",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPSC_CPSC-Extra.csv\n",
      "PTB_PTBXL.csv\n",
      "ChapmanShaoxing_Ningbo.csv\n",
      "G12EC.csv\n",
      "INCART.csv\n"
     ]
    }
   ],
   "source": [
    "# DB-wise CSV files (only the original ones)\n",
    "csv_files = []\n",
    "for file in os.listdir(csv_path):\n",
    "    if not file.startswith('.'):\n",
    "        chars = [c for c in file]\n",
    "        if chars.count('_') <= 1:\n",
    "            csv_files.append(file)\n",
    "\n",
    "print(*csv_files, sep='\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0548bf30",
   "metadata": {},
   "source": [
    "## <font color = teal> Combinations of Training, Validation and Testing sets </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a20b7fe9",
   "metadata": {},
   "source": [
    "Yaml files are created based on the csv files listed above. Yaml files can be divided into training, validation and testing yaml files. All the possible splits are as follows\n",
    "\n",
    "**CPSC_CPSC-Extra.csv as test data**\n",
    "\n",
    "    1) train: PTB_PTBXL.csv, INCART.csv, G12EC.csv\n",
    "       val: ChapmanShaoxing_Ningbo.csv\n",
    "\n",
    "    2) train: PTB_PTBXL.csv, INCART.csv, ChapmanShaoxing_Ningbo.csv\n",
    "       val: G12EC.csv\n",
    "\n",
    "    3) train: PTB_PTBXL.csv, ChapmanShaoxing_Ningbo.csv, G12EC.csv\n",
    "       val: INCART.csv\n",
    "\n",
    "    4) train: INCART.csv, ChapmanShaoxing_Ningbo.csv, G12EC.csv\n",
    "       val: PTB_PTBXL.csv\n",
    "\n",
    "**ChapmanShaoxing_Ningbo.csv as test data**\n",
    "\n",
    "    1) train: PTB_PTBXL.csv, INCART.csv, G12EC.csv\n",
    "       val: CPSC_CPSC-Extra.csv\n",
    "\n",
    "    2) train: PTB_PTBXL.csv, INCART.csv, CPSC_CPSC-Extra.csv\n",
    "       val: G12EC.csv\n",
    "\n",
    "    3) train: PTB_PTBXL.csv, CPSC_CPSC-Extra.csv, G12EC.csv\n",
    "       val: INCART.csv\n",
    "\n",
    "    4) train: INCART.csv, CPSC_CPSC-Extra.csv, G12EC.csv\n",
    "       val: PTB_PTBXL.csv, \n",
    "\n",
    "**PTB_PTBXL.csv as test data**\n",
    "\n",
    "    1) train: ChapmanShaoxing_Ningbo.csv, INCART.csv, G12EC.csv\n",
    "       val: CPSC_CPSC-Extra.csv\n",
    "\n",
    "    2) train: ChapmanShaoxing_Ningbo.csv, INCART.csv, CPSC_CPSC-Extra.csv\n",
    "       val: G12EC.csv\n",
    "\n",
    "    3) train: ChapmanShaoxing_Ningbo.csv, CPSC_CPSC-Extra.csv, G12EC.csv\n",
    "       val: INCART.csv\n",
    "\n",
    "    4) train: INCART.csv, CPSC_CPSC-Extra.csv, G12EC.csv\n",
    "       val: ChapmanShaoxing_Ningbo.csv\n",
    "\n",
    "**INCART.csv as test data**\n",
    "\n",
    "    1) train: ChapmanShaoxing_Ningbo.csv, PTB_PTBXL.csv, G12EC.csv\n",
    "       val: CPSC_CPSC-Extra.csv\n",
    "\n",
    "    2) train: ChapmanShaoxing_Ningbo.csv, PTB_PTBXL.csv, CPSC_CPSC-Extra.csv\n",
    "       val: G12EC.csv\n",
    "\n",
    "    3) train: ChapmanShaoxing_Ningbo.csv, CPSC_CPSC-Extra.csv, G12EC.csv\n",
    "       val: PTB_PTBXL.csv\n",
    "\n",
    "    4) train: PTB_PTBXL.csv, CPSC_CPSC-Extra.csv, G12EC.csv\n",
    "       val: ChapmanShaoxing_Ningbo.csv\n",
    "\n",
    "**G12EC.csv as test data**\n",
    "\n",
    "    1) train: ChapmanShaoxing_Ningbo.csv, PTB_PTBXL.csv, INCART.csv\n",
    "       val: CPSC_CPSC-Extra.csv\n",
    "\n",
    "    2) train: ChapmanShaoxing_Ningbo.csv, PTB_PTBXL.csv, CPSC_CPSC-Extra.csv\n",
    "       val: INCART.csv\n",
    "\n",
    "    3) train: ChapmanShaoxing_Ningbo.csv, CPSC_CPSC-Extra.csv, INCART.csv\n",
    "       val: PTB_PTBXL.csv\n",
    "\n",
    "    4) train: PTB_PTBXL.csv, CPSC_CPSC-Extra.csv, INCART.csv\n",
    "       val: ChapmanShaoxing_Ningbo.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2775f96",
   "metadata": {},
   "source": [
    "Let's make a function to find the combinations which are set in the `training_data` attribute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9b4c3f9e",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['PTB_PTBXL.csv', 'INCART.csv', 'G12EC.csv'], ['ChapmanShaoxing_Ningbo.csv'], ['CPSC_CPSC-Extra.csv']]\n",
      "[['PTB_PTBXL.csv', 'INCART.csv', 'ChapmanShaoxing_Ningbo.csv'], ['G12EC.csv'], ['CPSC_CPSC-Extra.csv']]\n",
      "[['PTB_PTBXL.csv', 'G12EC.csv', 'ChapmanShaoxing_Ningbo.csv'], ['INCART.csv'], ['CPSC_CPSC-Extra.csv']]\n",
      "[['INCART.csv', 'G12EC.csv', 'ChapmanShaoxing_Ningbo.csv'], ['PTB_PTBXL.csv'], ['CPSC_CPSC-Extra.csv']]\n",
      "The length of the first set: 4\n"
     ]
    }
   ],
   "source": [
    "from itertools import combinations\n",
    "\n",
    "def different_combinations(files):\n",
    "    '''Every combination of the files for train/val split'''\n",
    "    \n",
    "    all_combs = []\n",
    "    for combs in combinations(files, 4):\n",
    "        for c in combinations(combs, 3):\n",
    "            train_tmp = list(c)\n",
    "            val_tmp = [file for file in files if file not in c]\n",
    "            train_val = [train_tmp, val_tmp]\n",
    "            all_combs.append(train_val)\n",
    "    \n",
    "    return all_combs\n",
    "\n",
    "\n",
    "combinations_data = []\n",
    "for data in training_data:\n",
    "    combs_tmp = different_combinations(data)\n",
    "    combinations_data.append(combs_tmp)\n",
    "\n",
    "train_val_set = []\n",
    "# Find test data file for these so it's included neither in training nor validation data\n",
    "for i, data in enumerate(combinations_data):\n",
    "    for train_val_set in data:\n",
    "        train_val_files = train_val_set[0] + train_val_set[1]\n",
    "        test_file = [os.path.basename(file) for file in csv_files if os.path.basename(file) not in train_val_files]\n",
    "        train_val_set.append(test_file) \n",
    "\n",
    "# For example, data_1\n",
    "print(*combinations_data[0], sep='\\n')\n",
    "print('The length of the first set:', len(combinations_data[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3539b31",
   "metadata": {},
   "source": [
    "All the different training, validation and testing splits are stored in the `combinations_data` attribute. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81e08de4",
   "metadata": {},
   "source": [
    "## <font color = teal> Combined CSV files and Yaml files </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5545131",
   "metadata": {},
   "source": [
    "Let's make the combined csv files as the yaml files in the training phase use two attributes --- `train_file` and `val_file` ---, and all the information of training data should be found from one csv file. I.e., all ECGs in training should be listed in one csv file, and respectively, all ECGs for validation should be listed in another csv file, as well as the ECGs for testing. All the yaml files for training will be saved to `/configs/training`. The yaml files for testing will be saved to `/configs/predicting`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a317ee68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4 names created, for example ['split_1.yaml', 'split_2.yaml', 'split_3.yaml']\n"
     ]
    }
   ],
   "source": [
    "# Let's start by gathering names for the yaml files to a list\n",
    "\n",
    "# Counting all the elements in combinations_data\n",
    "element_count = sum([len(elem) for elem in combinations_data]) \n",
    "\n",
    "# Create as many names as there are elements in combinations_data\n",
    "split_names = []\n",
    "for i in range(0, element_count):\n",
    "    split_names.append(name + str(i+1) + '.yaml')  \n",
    "    \n",
    "print(f'{len(split_names)} names created, for example {split_names[:3]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "bfa36ae7",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving combined training data as PTB_PTBXL_INCART_G12EC.csv with a length of 60.\n",
      "Validation data is from ChapmanShaoxing_Ningbo.csv\n",
      "Testing data is from CPSC_CPSC-Extra.csv\n",
      "\n",
      "Saving combined training data as PTB_PTBXL_INCART_ChapmanShaoxing_Ningbo.csv with a length of 66.\n",
      "Validation data is from G12EC.csv\n",
      "Testing data is from CPSC_CPSC-Extra.csv\n",
      "\n",
      "Saving combined training data as PTB_PTBXL_G12EC_ChapmanShaoxing_Ningbo.csv with a length of 68.\n",
      "Validation data is from INCART.csv\n",
      "Testing data is from CPSC_CPSC-Extra.csv\n",
      "\n",
      "Saving combined training data as INCART_G12EC_ChapmanShaoxing_Ningbo.csv with a length of 58.\n",
      "Validation data is from PTB_PTBXL.csv\n",
      "Testing data is from CPSC_CPSC-Extra.csv\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from ruamel.yaml import YAML\n",
    " \n",
    "def save_yaml(yaml_str, yaml_path, i):\n",
    "    ''' Save the given string as a yaml file in the given location.\n",
    "    '''\n",
    "    # Make the yaml directory\n",
    "    if not os.path.isdir(yaml_path):\n",
    "        os.mkdir(yaml_path)\n",
    "    \n",
    "    # Write the yaml file\n",
    "    with open(os.path.join(yaml_path, split_names[i] ), 'w') as yaml_file:\n",
    "        yaml = YAML()\n",
    "        code = yaml.load(yaml_str)\n",
    "        yaml.dump(code, yaml_file)\n",
    "    \n",
    "        \n",
    "def create_testing_yaml(test_csv, i):\n",
    "    ''' Make a yaml file for prediction. The base of it is presented above.\n",
    "    '''\n",
    "    \n",
    "    model_name = split_names[i].split('.')[0] + '.pth'\n",
    "    yaml_str = '''\\\n",
    "# INITIAL SETTINGS\n",
    "    test_file: {}\n",
    "    model: {}\n",
    "    \n",
    "# TESTING SETTINGS\n",
    "    threshold: {:f}\n",
    "\n",
    "# DEVICE CONFIGS\n",
    "    device_count: {}  \n",
    "    '''.format(test_csv,\n",
    "               model_name,\n",
    "               threshold,\n",
    "               device_count)\n",
    "    \n",
    "    yaml_path = test_yaml_save_path\n",
    "    save_yaml(yaml_str, yaml_path, i)\n",
    "    \n",
    "\n",
    "def create_training_yaml(train_csv, val_csv, i):\n",
    "    ''' Make a yaml file for training. The base of it is presented above.\n",
    "    '''\n",
    "    yaml_str = '''\\\n",
    "# INITIAL SETTINGS\n",
    "    train_file: {}\n",
    "    val_file: {}\n",
    "\n",
    "# TRAINING SETTINGS\n",
    "    batch_size: {}\n",
    "    num_workers: {}\n",
    "    epochs: {}\n",
    "    lr: {:f}\n",
    "    weight_decay: {:f}\n",
    "\n",
    "# DEVICE CONFIGS\n",
    "    device_count: {}   \n",
    "    '''.format(train_csv,\n",
    "               val_csv,\n",
    "               batch_size,\n",
    "               num_workers, \n",
    "               epochs,\n",
    "               lr,\n",
    "               weight_decay,\n",
    "               device_count)\n",
    "    \n",
    "    yaml_path = train_yaml_save_path\n",
    "    save_yaml(yaml_str, yaml_path, i)\n",
    "    \n",
    "        \n",
    "def combine_csv(files, i):\n",
    "    '''Combine all files in the list of train csv files. Save the result as a csv file.\n",
    "    ''' \n",
    "\n",
    "    # As we con't have csv files of combined databases for training let's make them\n",
    "    train_csv_name = [os.path.basename(file).split('.')[0] for file in files[0]]\n",
    "    train_csv_name = '_'.join(train_csv_name) + '.csv'\n",
    "    train_files = [os.path.join(csv_path, f) for f in files[0]]\n",
    "    combined_train_csv = pd.concat([pd.read_csv(f) for f in train_files], ignore_index = True)\n",
    "    print('Saving combined training data as', train_csv_name, 'with a length of {}.'.format(len(combined_train_csv)))\n",
    "    \n",
    "    # Saving a csv file to the same location where \n",
    "    # all the database-wise splitted csv files are\n",
    "    combined_train_csv.to_csv(os.path.join(csv_path, train_csv_name), sep=',', index=False)\n",
    "    \n",
    "    # Now we got the csv file for training data, e.g., PTB_PTBXL_INCART_G12EC.csv\n",
    "    # Validation file is simply\n",
    "    val_csv = ''.join(files[1])\n",
    "    print('Validation data is from', val_csv)\n",
    "    create_training_yaml(train_csv_name, val_csv, i)\n",
    "\n",
    "    # Lastly, the prediction yaml files\n",
    "    pred_csv = ''.join(files[2])\n",
    "    print('Testing data is from', pred_csv)\n",
    "    create_testing_yaml(pred_csv, i)\n",
    "    \n",
    "split_i = 0 \n",
    "for data in combinations_data:\n",
    "    for train_val_set in data:\n",
    "        combine_csv(train_val_set, split_i)\n",
    "        split_i += 1\n",
    "        print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4622c35",
   "metadata": {},
   "source": [
    "Now all the yaml files for training, validation and testing are created! The training yaml files are located in `/configs/training/train_DBwise_smoke/` and the testing yaml files in `/configs/predicting/predict_DBwise_smoke/`. There are also the combined csv files for ECGs created in `/data/split_csvs/physionet_DBwise_smoke/`.\n",
    "\n",
    "<font color=red>**NOTE 1!**</font> It is extremely important that in the testing yaml file the model is set with the same name as the yaml file which the model is trained with. E.g. when a model is trained using `split_1.yaml`, it will be saved as `split_1.pth`. This makes using the repository much easier and simpler. Mind this, if you want to edit the code below.\n",
    "\n",
    "<font color=red>**NOTE 2!**</font> If you are now wondering why the yaml files don't have the csv files in single quotation marks, it's ok. Scripts are able to read and load the values from the yaml files even without those marks."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "vscode": {
   "interpreter": {
    "hash": "46963f0590c8313baefd57cb1336ee0094ee3a6da0b1eb974571013be2a14d92"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
