{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7999d565",
   "metadata": {},
   "source": [
    "# Yaml files of Database-wise Split for Training and Prediction\n",
    "\n",
    "With this notebook, you can create the yaml files needed in training and prediction with a data split which is made database-wise. The so-called \"original data split\" should be first made as with the script `prepare_data.py`. More detailed information about this is available in the notebook [Introductions for Data Handling](1_introductions_data_handling.ipynb). \n",
    "\n",
    "------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d2f21607",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# PARAMETERS TO CREATE DBWISE YAML FILES  \n",
    "# ----------------------------------------\n",
    "\n",
    "# From where to load the csv files of \"original data split\"\n",
    "csv_path = os.path.join('../data/split_csvs/', 'physionet_DBwise_smoke')\n",
    "\n",
    "# Where to save combined training csv files\n",
    "# e.g. we want to use PTB_PTBXL.csv, INCART.csv and G12EC.csv for training\n",
    "#      so we need to combine them first for the model to use them\n",
    "combined_save_path = os.path.join('../data/split_csvs', 'physionet_DBwise_smoke')\n",
    "\n",
    "# Where to save the training yaml files\n",
    "train_yaml_save_path = os.path.join('../configs/training', 'train_DBwise_smoke')\n",
    "\n",
    "# Where to save the testing yaml files\n",
    "test_yaml_save_path = os.path.join('../configs/predicting', 'prediction_DBwise_smoke')\n",
    "\n",
    "# The files which need to be split into training and validation data\n",
    "# We have 5 different databases so 5 different train/val split sets\n",
    "training_data = [['PTB_PTBXL.csv', 'INCART.csv', 'G12EC.csv', 'ChapmanShaoxing_Ningbo.csv']]\n",
    "\n",
    "# !! All the other splits -> just add to the list above if wanted\n",
    "# ['PTB_PTBXL.csv', 'INCART.csv', 'G12EC.csv', 'CPSC_CPSC-Extra.csv'],\n",
    "# ['PTB_PTBXL.csv', 'INCART.csv', 'CPSC_CPSC-Extra.csv', 'ChapmanShaoxing_Ningbo.csv'],\n",
    "# ['PTB_PTBXL.csv', 'CPSC_CPSC-Extra.csv', 'G12EC.csv', 'ChapmanShaoxing_Ningbo.csv'],\n",
    "# ['INCART.csv', 'CPSC_CPSC-Extra.csv', 'G12EC.csv','ChapmanShaoxing_Ningbo.csv']\n",
    "\n",
    "# Name for yaml files given as a string\n",
    "# names will be formed as <name><index>.yaml\n",
    "name = 'split'\n",
    "\n",
    "# --- Parameters for training yaml files -------------\n",
    "batch_size = 10\n",
    "num_workers = 0\n",
    "epochs = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ced2ffe0",
   "metadata": {},
   "source": [
    "First, we need to find the csv files from which we want to create the yaml files. They should be located in `/data/split_csvs/`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f005f047",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPSC_CPSC-Extra.csv\n",
      "PTB_PTBXL.csv\n",
      "ChapmanShaoxing_Ningbo.csv\n",
      "G12EC.csv\n",
      "INCART.csv\n"
     ]
    }
   ],
   "source": [
    "# DB-wise CSV files (only the original ones)\n",
    "csv_files = []\n",
    "for file in os.listdir(csv_path):\n",
    "    if not file.startswith('.'):\n",
    "        chars = [c for c in file]\n",
    "        if chars.count('_') <= 1:\n",
    "            csv_files.append(file)\n",
    "\n",
    "print(*csv_files, sep='\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0548bf30",
   "metadata": {},
   "source": [
    "## Combinations of Training and Validation Files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a20b7fe9",
   "metadata": {},
   "source": [
    "Out of the above listed csv files we want to make the database-wise splits for training, validation and prediction. So all the possible splits are as follows\n",
    "\n",
    "```\n",
    "1) CPSC_CPSC-Extra.csv for test data\n",
    "\n",
    "train: PTB_PTBXL.csv, INCART.csv, G12EC.csv\n",
    "val: ChapmanShaoxing_Ningbo.csv\n",
    "test: CPSC_CPSC-Extra.csv\n",
    "\n",
    "train: PTB_PTBXL.csv, INCART.csv, ChapmanShaoxing_Ningbo.csv\n",
    "val: G12EC.csv\n",
    "test: CPSC_CPSC-Extra.csv\n",
    "\n",
    "train: PTB_PTBXL.csv, ChapmanShaoxing_Ningbo.csv, G12EC.csv\n",
    "val: INCART.csv\n",
    "test: CPSC_CPSC-Extra.csv\n",
    "\n",
    "train: INCART.csv, ChapmanShaoxing_Ningbo.csv, G12EC.csv\n",
    "val: PTB_PTBXL.csv, \n",
    "test: CPSC_CPSC-Extra.csv\n",
    "\n",
    "2) ChapmanShaoxing_Ningbo.csv for test data\n",
    "\n",
    "train: PTB_PTBXL.csv, INCART.csv, G12EC.csv\n",
    "val: CPSC_CPSC-Extra.csv\n",
    "test: ChapmanShaoxing_Ningbo.csv\n",
    "\n",
    "train: PTB_PTBXL.csv, INCART.csv, CPSC_CPSC-Extra.csv\n",
    "val: G12EC.csv\n",
    "test: ChapmanShaoxing_Ningbo.csv\n",
    "\n",
    "train: PTB_PTBXL.csv, CPSC_CPSC-Extra.csv, G12EC.csv\n",
    "val: INCART.csv\n",
    "test: ChapmanShaoxing_Ningbo.csv\n",
    "\n",
    "train: INCART.csv, CPSC_CPSC-Extra.csv, G12EC.csv\n",
    "val: PTB_PTBXL.csv, \n",
    "test: ChapmanShaoxing_Ningbo.csv\n",
    "\n",
    "3) PTB_PTBXL.csv for test data\n",
    "\n",
    "train: ChapmanShaoxing_Ningbo.csv, INCART.csv, G12EC.csv\n",
    "val: CPSC_CPSC-Extra.csv\n",
    "test: PTB_PTBXL.csv\n",
    "\n",
    "train: ChapmanShaoxing_Ningbo.csv, INCART.csv, CPSC_CPSC-Extra.csv\n",
    "val: G12EC.csv\n",
    "test: PTB_PTBXL.csv\n",
    "\n",
    "train: ChapmanShaoxing_Ningbo.csv, CPSC_CPSC-Extra.csv, G12EC.csv\n",
    "val: INCART.csv\n",
    "test: PTB_PTBXL.csv\n",
    "\n",
    "train: INCART.csv, CPSC_CPSC-Extra.csv, G12EC.csv\n",
    "val: ChapmanShaoxing_Ningbo.csv\n",
    "test: PTB_PTBXL.csv\n",
    "\n",
    "4) INCART.csv for test data\n",
    "\n",
    "train: ChapmanShaoxing_Ningbo.csv, PTB_PTBXL.csv, G12EC.csv\n",
    "val: CPSC_CPSC-Extra.csv\n",
    "test: INCART.csv\n",
    "\n",
    "train: ChapmanShaoxing_Ningbo.csv, PTB_PTBXL.csv, CPSC_CPSC-Extra.csv\n",
    "val: G12EC.csv\n",
    "test: INCART.csv\n",
    "\n",
    "train: ChapmanShaoxing_Ningbo.csv, CPSC_CPSC-Extra.csv, G12EC.csv\n",
    "val: PTB_PTBXL.csv\n",
    "test: INCART.csv\n",
    "\n",
    "train: PTB_PTBXL.csv, CPSC_CPSC-Extra.csv, G12EC.csv\n",
    "val: ChapmanShaoxing_Ningbo.csv\n",
    "test: INCART.csv\n",
    "\n",
    "5) G12EC.csv for test data\n",
    "\n",
    "train: ChapmanShaoxing_Ningbo.csv, PTB_PTBXL.csv, INCART.csv\n",
    "val: CPSC_CPSC-Extra.csv\n",
    "test: G12EC.csv\n",
    "\n",
    "train: ChapmanShaoxing_Ningbo.csv, PTB_PTBXL.csv, CPSC_CPSC-Extra.csv\n",
    "val: INCART.csv\n",
    "test: G12EC.csv\n",
    "\n",
    "train: ChapmanShaoxing_Ningbo.csv, CPSC_CPSC-Extra.csv, INCART.csv\n",
    "val: PTB_PTBXL.csv\n",
    "test: G12EC.csv\n",
    "\n",
    "train: PTB_PTBXL.csv, CPSC_CPSC-Extra.csv, INCART.csv\n",
    "val: ChapmanShaoxing_Ningbo.csv\n",
    "test: G12EC.csv\n",
    "\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2775f96",
   "metadata": {},
   "source": [
    "Let's make a function to find all these combinations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9b4c3f9e",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['PTB_PTBXL.csv', 'INCART.csv', 'G12EC.csv'], ['ChapmanShaoxing_Ningbo.csv'], ['CPSC_CPSC-Extra.csv']]\n",
      "[['PTB_PTBXL.csv', 'INCART.csv', 'ChapmanShaoxing_Ningbo.csv'], ['G12EC.csv'], ['CPSC_CPSC-Extra.csv']]\n",
      "[['PTB_PTBXL.csv', 'G12EC.csv', 'ChapmanShaoxing_Ningbo.csv'], ['INCART.csv'], ['CPSC_CPSC-Extra.csv']]\n",
      "[['INCART.csv', 'G12EC.csv', 'ChapmanShaoxing_Ningbo.csv'], ['PTB_PTBXL.csv'], ['CPSC_CPSC-Extra.csv']]\n",
      "The length of the first set: 4\n"
     ]
    }
   ],
   "source": [
    "from itertools import combinations\n",
    "\n",
    "def different_combinations(files):\n",
    "    '''Every combination of the files for train/val split'''\n",
    "    \n",
    "    all_combs = []\n",
    "    for combs in combinations(files, 4):\n",
    "        for c in combinations(combs, 3):\n",
    "            train_tmp = list(c)\n",
    "            val_tmp = [file for file in files if file not in c]\n",
    "            train_val = [train_tmp, val_tmp]\n",
    "            all_combs.append(train_val)\n",
    "    \n",
    "    return all_combs\n",
    "\n",
    "\n",
    "combinations_data = []\n",
    "for data in training_data:\n",
    "    combs_tmp = different_combinations(data)\n",
    "    combinations_data.append(combs_tmp)\n",
    "\n",
    "train_val_set = []\n",
    "# Find test data file for these so it's included neither in training nor validation data\n",
    "for i, data in enumerate(combinations_data):\n",
    "    for train_val_set in data:\n",
    "        train_val_files = train_val_set[0] + train_val_set[1]\n",
    "        test_file = [os.path.basename(file) for file in csv_files if os.path.basename(file) not in train_val_files]\n",
    "        train_val_set.append(test_file) \n",
    "\n",
    "# For example, data_1\n",
    "print(*combinations_data[0], sep='\\n')\n",
    "print('The length of the first set:', len(combinations_data[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3539b31",
   "metadata": {},
   "source": [
    "Now we have all the combinations of training and validation splits in `data_1`, `data_2` etc."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81e08de4",
   "metadata": {},
   "source": [
    "## Combined CSV files and Yaml files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5545131",
   "metadata": {},
   "source": [
    "Let's make the combined csv files since the yaml files in the training phase use two attributes --- `train_file` and `val_file:` --- so need to set the above information to them. All of these will be saved in the `/configs/training`. The yaml files for predictions are constructed to `/configs/testing`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eab04070",
   "metadata": {},
   "source": [
    "For reminder, the base of the training yaml files is as follows\n",
    "\n",
    "```\n",
    "# INITIAL SETTINGS\n",
    "train_file: PTB_PTBXL_INCART_G12EC.csv\n",
    "val_file: ChapmanShaoxing_Ningbo.csv\n",
    "\n",
    "# TRAINING SETTINGS\n",
    "batch_size: 10\n",
    "num_workers: 0\n",
    "\n",
    "# SAVE, LOAD AND DISPLAY INFORMATION\n",
    "epochs: 1\n",
    "\n",
    "```\n",
    "\n",
    "and the one of the prediction yaml files as follows\n",
    "\n",
    "```\n",
    "# INITIAL SETTINGS\n",
    "test_file: CPSC_CPSC-Extra.csv\n",
    "model: split0.pth\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bfa36ae7",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving combined training data as PTB_PTBXL_INCART_G12EC.csv with a length of 30.\n",
      "Validation data is from ChapmanShaoxing_Ningbo.csv\n",
      "Testing data is from CPSC_CPSC-Extra.csv\n",
      "\n",
      "Saving combined training data as PTB_PTBXL_INCART_ChapmanShaoxing_Ningbo.csv with a length of 33.\n",
      "Validation data is from G12EC.csv\n",
      "Testing data is from CPSC_CPSC-Extra.csv\n",
      "\n",
      "Saving combined training data as PTB_PTBXL_G12EC_ChapmanShaoxing_Ningbo.csv with a length of 34.\n",
      "Validation data is from INCART.csv\n",
      "Testing data is from CPSC_CPSC-Extra.csv\n",
      "\n",
      "Saving combined training data as INCART_G12EC_ChapmanShaoxing_Ningbo.csv with a length of 29.\n",
      "Validation data is from PTB_PTBXL.csv\n",
      "Testing data is from CPSC_CPSC-Extra.csv\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from ruamel.yaml import YAML\n",
    "import ruamel.yaml\n",
    " \n",
    "# Names for yaml files\n",
    "element_count = sum([len(elem) for elem in combinations_data]) # Counting all the elements in a list of lists\n",
    "split_names = []\n",
    "for i in range(element_count):\n",
    "    split_names.append(name + str(i) + '.yaml')    \n",
    "    \n",
    "def save_yaml(yaml_str, yaml_path, i):\n",
    "    ''' Save the given string as a yaml file in the given location.\n",
    "    '''\n",
    "    # Make the yaml directory\n",
    "    if not os.path.isdir(yaml_path):\n",
    "        os.mkdir(yaml_path)\n",
    "    \n",
    "    # Write the yaml file\n",
    "    with open(os.path.join(yaml_path, split_names[i] ), 'w') as yaml_file:\n",
    "        yaml = YAML()\n",
    "        code = yaml.load(yaml_str)\n",
    "        yaml.dump(code, yaml_file)\n",
    "    \n",
    "        \n",
    "def create_prediction_yaml(test_csv, i):\n",
    "    ''' Make a yaml file for prediction. The base of it is presented above.\n",
    "    '''\n",
    "    \n",
    "    model_name = split_names[i].split('.')[0] + '.pth'\n",
    "    yaml_str = '''\\\n",
    "# INITIAL SETTINGS\n",
    "    test_file: {}\n",
    "    model: {}\n",
    "    '''.format(test_csv, model_name)\n",
    "    yaml_path = test_yaml_save_path\n",
    "    save_yaml(yaml_str, yaml_path, i)\n",
    "    \n",
    "\n",
    "def create_training_yaml(train_csv, val_csv, i):\n",
    "    ''' Make a yaml file for training. The base of it is presented above.\n",
    "    '''\n",
    "    yaml_str = '''\\\n",
    "# INITIAL SETTINGS\n",
    "    train_file: {}\n",
    "    val_file: {}\n",
    "\n",
    "# TRAINING SETTINGS\n",
    "    batch_size: {}\n",
    "    num_workers: {}\n",
    "\n",
    "# SAVE, LOAD AND DISPLAY INFORMATION\n",
    "    epochs: {}\n",
    "    '''.format(train_csv, val_csv,\n",
    "              batch_size, num_workers, epochs)\n",
    "    yaml_path = train_yaml_save_path\n",
    "    save_yaml(yaml_str, yaml_path, i)\n",
    "    \n",
    "        \n",
    "def combine_csv(files, i):\n",
    "    '''Combine all files in the list of train csv files. Save the result as a csv file.\n",
    "    ''' \n",
    "    train_csv_name = [os.path.basename(file).split('.')[0] for file in files[0]]\n",
    "    train_csv_name = '_'.join(train_csv_name) + '.csv'\n",
    "    train_files = [os.path.join(csv_path, f) for f in files[0]]\n",
    "    combined_train_csv = pd.concat([pd.read_csv(f) for f in train_files], ignore_index = True)\n",
    "    \n",
    "    print('Saving combined training data as', train_csv_name, 'with a length of {}.'.format(len(combined_train_csv)))\n",
    "\n",
    "    save_path = combined_save_path\n",
    "    # Make the save directory\n",
    "    if not os.path.isdir(save_path):\n",
    "        os.mkdir(save_path)\n",
    "    \n",
    "    # Saving a csv file\n",
    "    combined_train_csv.to_csv(os.path.join(save_path, train_csv_name), sep=',', index=False)\n",
    "    \n",
    "    # Now we got the csv file for training data, e.g., PTB_PTBXL_INCART_G12EC.csv\n",
    "    # Validation file is simply\n",
    "    val_csv = ''.join(files[1])\n",
    "    print('Validation data is from', val_csv)\n",
    "    create_training_yaml(train_csv_name, val_csv, i)\n",
    "\n",
    "    # Lastly, the prediction yaml files\n",
    "    pred_csv = ''.join(files[2])\n",
    "    print('Testing data is from', pred_csv)\n",
    "    create_prediction_yaml(pred_csv, i)\n",
    "    \n",
    "split_i = 0  \n",
    "for data in combinations_data:\n",
    "    for train_val_set in data:\n",
    "        combine_csv(train_val_set, split_i)\n",
    "        split_i += 1\n",
    "        print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4622c35",
   "metadata": {},
   "source": [
    "Now all the yaml files for training, validation and prediction are created! The training yaml files are located in `/configs/training/train_DBwise_smoke/` and the prediction yaml files in `/configs/predicting/prediction_DBwise_smoke/`.\n",
    "\n",
    "<font color=red>**NOTE 1!**</font> It is extremely important that in the test yaml file the model is set with the same name as the yaml file which the model is trained with. E.g. when a model is trained using `split0.yaml`, it will be saved as `split0.pth`. This makes using the repository much easier and simpler. Mind this, if you want to edit the code below.\n",
    "\n",
    "<font color=red>**NOTE 2!**</font> If you are now wondering why the yaml files don't have the csv values in single quotation marks, it's ok. Scripts are able to read and load the values from the yaml files even without those marks!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
