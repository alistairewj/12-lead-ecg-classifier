{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7999d565",
   "metadata": {},
   "source": [
    "# Yaml files of Database-wise Split for Training and Testing\n",
    "\n",
    "With this notebook, you can create the yaml files needed in training and testing with a data split which is made **database-wise**. The so-called \"original data split\" should be first made with the script `create_data_split_csvs.py`. This ensures that there are already one csv file for each database. The combined csv files are created below. More detailed information about this is available in the notebook [Introduction for Data Handling](1_introduction_data_handling.ipynb). \n",
    "\n",
    "------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d2f21607",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "e:\\git\\12-lead-ecg-classifier\\data\\split_csvs\\physionet_DBwise_smoke\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# Absolute path of this file\n",
    "abs_path = Path(os.path.abspath(''))\n",
    "\n",
    "# PARAMETERS TO CREATE DBWISE YAML FILES  \n",
    "# ----------------------------------------\n",
    "\n",
    "# From where to load the csv files of \"original data split\"\n",
    "csv_path = os.path.join(abs_path.parent.absolute(), 'data', 'split_csvs', 'physionet_DBwise_smoke')\n",
    "print(csv_path)\n",
    "\n",
    "# Where to save combined training csv files\n",
    "# e.g. we want to use PTB_PTBXL.csv, INCART.csv and G12EC.csv for training\n",
    "#      so we need to combine them first for the model to use them\n",
    "combined_save_path = os.path.join(abs_path.parent.absolute(), 'data', 'split_csvs', 'physionet_DBwise_smoke')\n",
    "\n",
    "# Where to save the training yaml files\n",
    "train_yaml_save_path = os.path.join(abs_path.parent.absolute(), 'configs', 'training', 'train_DBwise_smoke')\n",
    "\n",
    "# Where to save the testing yaml files\n",
    "test_yaml_save_path = os.path.join(abs_path.parent.absolute(), 'configs', 'predicting', 'prediction_DBwise_smoke')\n",
    "\n",
    "# The files which need to be split into training and validation data\n",
    "# We have 5 different databases so 5 different train/val split sets\n",
    "training_data = [['PTB_PTBXL.csv', 'INCART.csv', 'G12EC.csv', 'ChapmanShaoxing_Ningbo.csv']]\n",
    "\n",
    "# !! All the other splits -> just add to the list above if wanted\n",
    "# ['PTB_PTBXL.csv', 'INCART.csv', 'G12EC.csv', 'CPSC_CPSC-Extra.csv'],\n",
    "# ['PTB_PTBXL.csv', 'INCART.csv', 'CPSC_CPSC-Extra.csv', 'ChapmanShaoxing_Ningbo.csv'],\n",
    "# ['PTB_PTBXL.csv', 'CPSC_CPSC-Extra.csv', 'G12EC.csv', 'ChapmanShaoxing_Ningbo.csv'],\n",
    "# ['INCART.csv', 'CPSC_CPSC-Extra.csv', 'G12EC.csv','ChapmanShaoxing_Ningbo.csv']\n",
    "\n",
    "# Name for yaml files given as a string\n",
    "# names will be formed as <name><index>.yaml\n",
    "name = 'split'\n",
    "\n",
    "# --- Parameters for training yaml files -------------\n",
    "batch_size = 10\n",
    "num_workers = 0\n",
    "epochs = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ced2ffe0",
   "metadata": {},
   "source": [
    "First, the csv files, from which the yaml files are created, need to be found. They should be located in `/data/split_csvs/`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f005f047",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChapmanShaoxing_Ningbo.csv\n",
      "CPSC_CPSC-Extra.csv\n",
      "G12EC.csv\n",
      "INCART.csv\n",
      "PTB_PTBXL.csv\n"
     ]
    }
   ],
   "source": [
    "# DB-wise CSV files (only the original ones)\n",
    "csv_files = []\n",
    "for file in os.listdir(csv_path):\n",
    "    if not file.startswith('.'):\n",
    "        chars = [c for c in file]\n",
    "        if chars.count('_') <= 1:\n",
    "            csv_files.append(file)\n",
    "\n",
    "print(*csv_files, sep='\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0548bf30",
   "metadata": {},
   "source": [
    "## Combinations of Training and Validation Files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a20b7fe9",
   "metadata": {},
   "source": [
    "Yaml files are created based on the csv files listed above. Yaml files can be divided into training, validation and testing yaml files. All the possible splits are as follows\n",
    "\n",
    "```\n",
    "1) CPSC_CPSC-Extra.csv for test data\n",
    "\n",
    "train: PTB_PTBXL.csv, INCART.csv, G12EC.csv\n",
    "val: ChapmanShaoxing_Ningbo.csv\n",
    "test: CPSC_CPSC-Extra.csv\n",
    "\n",
    "train: PTB_PTBXL.csv, INCART.csv, ChapmanShaoxing_Ningbo.csv\n",
    "val: G12EC.csv\n",
    "test: CPSC_CPSC-Extra.csv\n",
    "\n",
    "train: PTB_PTBXL.csv, ChapmanShaoxing_Ningbo.csv, G12EC.csv\n",
    "val: INCART.csv\n",
    "test: CPSC_CPSC-Extra.csv\n",
    "\n",
    "train: INCART.csv, ChapmanShaoxing_Ningbo.csv, G12EC.csv\n",
    "val: PTB_PTBXL.csv, \n",
    "test: CPSC_CPSC-Extra.csv\n",
    "\n",
    "2) ChapmanShaoxing_Ningbo.csv for test data\n",
    "\n",
    "train: PTB_PTBXL.csv, INCART.csv, G12EC.csv\n",
    "val: CPSC_CPSC-Extra.csv\n",
    "test: ChapmanShaoxing_Ningbo.csv\n",
    "\n",
    "train: PTB_PTBXL.csv, INCART.csv, CPSC_CPSC-Extra.csv\n",
    "val: G12EC.csv\n",
    "test: ChapmanShaoxing_Ningbo.csv\n",
    "\n",
    "train: PTB_PTBXL.csv, CPSC_CPSC-Extra.csv, G12EC.csv\n",
    "val: INCART.csv\n",
    "test: ChapmanShaoxing_Ningbo.csv\n",
    "\n",
    "train: INCART.csv, CPSC_CPSC-Extra.csv, G12EC.csv\n",
    "val: PTB_PTBXL.csv, \n",
    "test: ChapmanShaoxing_Ningbo.csv\n",
    "\n",
    "3) PTB_PTBXL.csv for test data\n",
    "\n",
    "train: ChapmanShaoxing_Ningbo.csv, INCART.csv, G12EC.csv\n",
    "val: CPSC_CPSC-Extra.csv\n",
    "test: PTB_PTBXL.csv\n",
    "\n",
    "train: ChapmanShaoxing_Ningbo.csv, INCART.csv, CPSC_CPSC-Extra.csv\n",
    "val: G12EC.csv\n",
    "test: PTB_PTBXL.csv\n",
    "\n",
    "train: ChapmanShaoxing_Ningbo.csv, CPSC_CPSC-Extra.csv, G12EC.csv\n",
    "val: INCART.csv\n",
    "test: PTB_PTBXL.csv\n",
    "\n",
    "train: INCART.csv, CPSC_CPSC-Extra.csv, G12EC.csv\n",
    "val: ChapmanShaoxing_Ningbo.csv\n",
    "test: PTB_PTBXL.csv\n",
    "\n",
    "4) INCART.csv for test data\n",
    "\n",
    "train: ChapmanShaoxing_Ningbo.csv, PTB_PTBXL.csv, G12EC.csv\n",
    "val: CPSC_CPSC-Extra.csv\n",
    "test: INCART.csv\n",
    "\n",
    "train: ChapmanShaoxing_Ningbo.csv, PTB_PTBXL.csv, CPSC_CPSC-Extra.csv\n",
    "val: G12EC.csv\n",
    "test: INCART.csv\n",
    "\n",
    "train: ChapmanShaoxing_Ningbo.csv, CPSC_CPSC-Extra.csv, G12EC.csv\n",
    "val: PTB_PTBXL.csv\n",
    "test: INCART.csv\n",
    "\n",
    "train: PTB_PTBXL.csv, CPSC_CPSC-Extra.csv, G12EC.csv\n",
    "val: ChapmanShaoxing_Ningbo.csv\n",
    "test: INCART.csv\n",
    "\n",
    "5) G12EC.csv for test data\n",
    "\n",
    "train: ChapmanShaoxing_Ningbo.csv, PTB_PTBXL.csv, INCART.csv\n",
    "val: CPSC_CPSC-Extra.csv\n",
    "test: G12EC.csv\n",
    "\n",
    "train: ChapmanShaoxing_Ningbo.csv, PTB_PTBXL.csv, CPSC_CPSC-Extra.csv\n",
    "val: INCART.csv\n",
    "test: G12EC.csv\n",
    "\n",
    "train: ChapmanShaoxing_Ningbo.csv, CPSC_CPSC-Extra.csv, INCART.csv\n",
    "val: PTB_PTBXL.csv\n",
    "test: G12EC.csv\n",
    "\n",
    "train: PTB_PTBXL.csv, CPSC_CPSC-Extra.csv, INCART.csv\n",
    "val: ChapmanShaoxing_Ningbo.csv\n",
    "test: G12EC.csv\n",
    "\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2775f96",
   "metadata": {},
   "source": [
    "Let's make a function to find the combinations which are set in the `training_data` attribute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9b4c3f9e",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['PTB_PTBXL.csv', 'INCART.csv', 'G12EC.csv'], ['ChapmanShaoxing_Ningbo.csv'], ['CPSC_CPSC-Extra.csv']]\n",
      "[['PTB_PTBXL.csv', 'INCART.csv', 'ChapmanShaoxing_Ningbo.csv'], ['G12EC.csv'], ['CPSC_CPSC-Extra.csv']]\n",
      "[['PTB_PTBXL.csv', 'G12EC.csv', 'ChapmanShaoxing_Ningbo.csv'], ['INCART.csv'], ['CPSC_CPSC-Extra.csv']]\n",
      "[['INCART.csv', 'G12EC.csv', 'ChapmanShaoxing_Ningbo.csv'], ['PTB_PTBXL.csv'], ['CPSC_CPSC-Extra.csv']]\n",
      "The length of the first set: 4\n"
     ]
    }
   ],
   "source": [
    "from itertools import combinations\n",
    "\n",
    "def different_combinations(files):\n",
    "    '''Every combination of the files for train/val split'''\n",
    "    \n",
    "    all_combs = []\n",
    "    for combs in combinations(files, 4):\n",
    "        for c in combinations(combs, 3):\n",
    "            train_tmp = list(c)\n",
    "            val_tmp = [file for file in files if file not in c]\n",
    "            train_val = [train_tmp, val_tmp]\n",
    "            all_combs.append(train_val)\n",
    "    \n",
    "    return all_combs\n",
    "\n",
    "\n",
    "combinations_data = []\n",
    "for data in training_data:\n",
    "    combs_tmp = different_combinations(data)\n",
    "    combinations_data.append(combs_tmp)\n",
    "\n",
    "train_val_set = []\n",
    "# Find test data file for these so it's included neither in training nor validation data\n",
    "for i, data in enumerate(combinations_data):\n",
    "    for train_val_set in data:\n",
    "        train_val_files = train_val_set[0] + train_val_set[1]\n",
    "        test_file = [os.path.basename(file) for file in csv_files if os.path.basename(file) not in train_val_files]\n",
    "        train_val_set.append(test_file) \n",
    "\n",
    "# For example, data_1\n",
    "print(*combinations_data[0], sep='\\n')\n",
    "print('The length of the first set:', len(combinations_data[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3539b31",
   "metadata": {},
   "source": [
    "All the different training, validation and testing splits are stored in the `combinations_data` attribute. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81e08de4",
   "metadata": {},
   "source": [
    "## Combined CSV files and Yaml files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5545131",
   "metadata": {},
   "source": [
    "Let's make the combined csv files as the yaml files in the training phase use two attributes --- `train_file` and `val_file` ---, and all the information of training data should be found from one csv file. I.e., all ECGs in training should be listed in one csv file, and respectively, all ECGs for validation should be listed in another csv file, as well as the ECGs for testing. All the yaml files for training will be saved to `/configs/training`. The yaml files for testing will be saved to `/configs/predicting`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eab04070",
   "metadata": {},
   "source": [
    "The base of the training yaml files is as follows\n",
    "\n",
    "```\n",
    "# INITIAL SETTINGS\n",
    "train_file: PTB_PTBXL_INCART_G12EC.csv\n",
    "val_file: ChapmanShaoxing_Ningbo.csv\n",
    "\n",
    "# TRAINING SETTINGS\n",
    "batch_size: 10\n",
    "num_workers: 0\n",
    "\n",
    "# SAVE, LOAD AND DISPLAY INFORMATION\n",
    "epochs: 1\n",
    "\n",
    "```\n",
    "\n",
    "and the one of the testing yaml files as follows\n",
    "\n",
    "```\n",
    "# INITIAL SETTINGS\n",
    "test_file: CPSC_CPSC-Extra.csv\n",
    "model: split0.pth\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bfa36ae7",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving combined training data as PTB_PTBXL_INCART_G12EC.csv with a length of 30.\n",
      "Validation data is from ChapmanShaoxing_Ningbo.csv\n",
      "Testing data is from CPSC_CPSC-Extra.csv\n",
      "\n",
      "Saving combined training data as PTB_PTBXL_INCART_ChapmanShaoxing_Ningbo.csv with a length of 33.\n",
      "Validation data is from G12EC.csv\n",
      "Testing data is from CPSC_CPSC-Extra.csv\n",
      "\n",
      "Saving combined training data as PTB_PTBXL_G12EC_ChapmanShaoxing_Ningbo.csv with a length of 34.\n",
      "Validation data is from INCART.csv\n",
      "Testing data is from CPSC_CPSC-Extra.csv\n",
      "\n",
      "Saving combined training data as INCART_G12EC_ChapmanShaoxing_Ningbo.csv with a length of 29.\n",
      "Validation data is from PTB_PTBXL.csv\n",
      "Testing data is from CPSC_CPSC-Extra.csv\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from ruamel.yaml import YAML\n",
    " \n",
    "# Names for yaml files\n",
    "element_count = sum([len(elem) for elem in combinations_data]) # Counting all the elements in a list of lists\n",
    "split_names = []\n",
    "for i in range(element_count):\n",
    "    split_names.append(name + str(i) + '.yaml')    \n",
    "    \n",
    "def save_yaml(yaml_str, yaml_path, i):\n",
    "    ''' Save the given string as a yaml file in the given location.\n",
    "    '''\n",
    "    # Make the yaml directory\n",
    "    if not os.path.isdir(yaml_path):\n",
    "        os.mkdir(yaml_path)\n",
    "    \n",
    "    # Write the yaml file\n",
    "    with open(os.path.join(yaml_path, split_names[i] ), 'w') as yaml_file:\n",
    "        yaml = YAML()\n",
    "        code = yaml.load(yaml_str)\n",
    "        yaml.dump(code, yaml_file)\n",
    "    \n",
    "        \n",
    "def create_prediction_yaml(test_csv, i):\n",
    "    ''' Make a yaml file for prediction. The base of it is presented above.\n",
    "    '''\n",
    "    \n",
    "    model_name = split_names[i].split('.')[0] + '.pth'\n",
    "    yaml_str = '''\\\n",
    "# INITIAL SETTINGS\n",
    "    test_file: {}\n",
    "    model: {}\n",
    "    '''.format(test_csv, model_name)\n",
    "    yaml_path = test_yaml_save_path\n",
    "    save_yaml(yaml_str, yaml_path, i)\n",
    "    \n",
    "\n",
    "def create_training_yaml(train_csv, val_csv, i):\n",
    "    ''' Make a yaml file for training. The base of it is presented above.\n",
    "    '''\n",
    "    yaml_str = '''\\\n",
    "# INITIAL SETTINGS\n",
    "    train_file: {}\n",
    "    val_file: {}\n",
    "\n",
    "# TRAINING SETTINGS\n",
    "    batch_size: {}\n",
    "    num_workers: {}\n",
    "\n",
    "# SAVE, LOAD AND DISPLAY INFORMATION\n",
    "    epochs: {}\n",
    "    '''.format(train_csv, val_csv,\n",
    "              batch_size, num_workers, epochs)\n",
    "    yaml_path = train_yaml_save_path\n",
    "    save_yaml(yaml_str, yaml_path, i)\n",
    "    \n",
    "        \n",
    "def combine_csv(files, i):\n",
    "    '''Combine all files in the list of train csv files. Save the result as a csv file.\n",
    "    ''' \n",
    "\n",
    "    # As we con't have csv files of combined databases for training let's make them\n",
    "    train_csv_name = [os.path.basename(file).split('.')[0] for file in files[0]]\n",
    "    train_csv_name = '_'.join(train_csv_name) + '.csv'\n",
    "    train_files = [os.path.join(csv_path, f) for f in files[0]]\n",
    "    combined_train_csv = pd.concat([pd.read_csv(f) for f in train_files], ignore_index = True)\n",
    "    print('Saving combined training data as', train_csv_name, 'with a length of {}.'.format(len(combined_train_csv)))\n",
    "\n",
    "    save_path = combined_save_path\n",
    "    # Make the save directory\n",
    "    if not os.path.isdir(save_path):\n",
    "        os.mkdir(save_path)\n",
    "    \n",
    "    # Saving a csv file\n",
    "    combined_train_csv.to_csv(os.path.join(save_path, train_csv_name), sep=',', index=False)\n",
    "    \n",
    "    # Now we got the csv file for training data, e.g., PTB_PTBXL_INCART_G12EC.csv\n",
    "    # Validation file is simply\n",
    "    val_csv = ''.join(files[1])\n",
    "    print('Validation data is from', val_csv)\n",
    "    create_training_yaml(train_csv_name, val_csv, i)\n",
    "\n",
    "    # Lastly, the prediction yaml files\n",
    "    pred_csv = ''.join(files[2])\n",
    "    print('Testing data is from', pred_csv)\n",
    "    create_prediction_yaml(pred_csv, i)\n",
    "    \n",
    "split_i = 0  \n",
    "for data in combinations_data:\n",
    "    for train_val_set in data:\n",
    "        combine_csv(train_val_set, split_i)\n",
    "        split_i += 1\n",
    "        print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4622c35",
   "metadata": {},
   "source": [
    "Now all the yaml files for training, validation and prediction are created! The training yaml files are located in `/configs/training/train_DBwise_smoke/` and the testing yaml files in `/configs/predicting/predict_DBwise_smoke/`. There are also the combined csv files for ECGs themselves created in `/data/split_csvs/physionet_DBwise_smoke/`.\n",
    "\n",
    "<font color=red>**NOTE 1!**</font> It is extremely important that in the testing yaml file the model is set with the same name as the yaml file which the model is trained with. E.g. when a model is trained using `split0.yaml`, it will be saved as `split0.pth`. This makes using the repository much easier and simpler. Mind this, if you want to edit the code below.\n",
    "\n",
    "<font color=red>**NOTE 2!**</font> If you are now wondering why the yaml files don't have the csv values in single quotation marks, it's ok. Scripts are able to read and load the values from the yaml files even without those marks!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('physionet')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "vscode": {
   "interpreter": {
    "hash": "46963f0590c8313baefd57cb1336ee0094ee3a6da0b1eb974571013be2a14d92"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
